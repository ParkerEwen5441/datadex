[
    {
        "Title": "Collecting Image Description Datasets using Crowdsourcing",
        "Abstract": "We describe our two new datasets with images described by humans. Both the datasets were collected using Amazon Mechanical Turk, a crowdsourcing platform. The two datasets contain significantly more descriptions per image than other existing datasets. One is based on a popular image description dataset called the UIUC Pascal Sentence Dataset, whereas the other is based on the Abstract Scenes dataset con- taining images made from clipart objects. In this paper we describe our interfaces, analyze some properties of and show example descriptions from our two datasets.",
        "Authors": [
            "Ramakrishna Vedantam",
            "C. Lawrence Zitnick",
            "Devi Parikh"
        ],
        "Date": "2014-11-12T01:34:46Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1411.3041v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "MS-COCO Dataset",
            "UIUC Pascal Sentence Dataset",
            "PASCAL-50S Dataset",
            "ABSTRACT-50S Dataset",
            "Image-Sentence Dataset",
            "Flickr8k Dataset",
            "Abstract Scenes Dataset"
        ],
        "Possible_Dataset_Links": []
    },
    {
        "Title": "Cross-dataset Training for Class Increasing Object Detection",
        "Abstract": "We present a conceptually simple, flexible and general framework for cross-dataset training in object detection. Given two or more already labeled datasets that target for different object classes, cross-dataset training aims to detect the union of the different classes, so that we do not have to label all the classes for all the datasets. By cross-dataset training, existing datasets can be utilized to detect the merged object classes with a single model. Further more, in industrial applications, the object classes usually increase on demand. So when adding new classes, it is quite time-consuming if we label the new classes on all the existing datasets. While using cross-dataset training, we only need to label the new classes on the new dataset. We experiment on PASCAL VOC, COCO, WIDER FACE and WIDER Pedestrian with both solo and cross-dataset settings. Results show that our cross-dataset pipeline can achieve similar impressive performance simultaneously on these datasets compared with training independently.",
        "Authors": [
            "Yongqiang Yao",
            "Yan Wang",
            "Yu Guo"
        ],
        "Date": "2020-01-14T04:40:47Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/2001.04621v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "WIDER Pedestrian Dataset",
            "Cross- Dataset",
            "Open Image Dataset",
            "Existing Dataset",
            "Two Dataset",
            "PASCAL VOC Dataset",
            "PASCAL VOC: PASCAL VOC Dataset",
            "Pedestrian Dataset",
            "Classes Dataset",
            "COCO Dataset",
            "Wider Face Dataset",
            "PED Dataset",
            "Hybrid Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://wider-challenge.org/,2018",
            "https://www.dropbox.com/s/jec"
        ]
    },
    {
        "Title": "Datasets on object manipulation and interaction: a survey",
        "Abstract": "A dataset is crucial for model learning and evaluation. Choosing the right dataset to use or making a new dataset requires the knowledge of those that are available. In this work, we provide that knowledge, by reviewing twenty datasets that were published in the recent six years and that are directly related to object manipulation. We report on modalities, activities, and annotations for each individual dataset and give our view on its use for object manipulation. We also compare the datasets and summarize them. We conclude with our suggestion on future datasets.",
        "Authors": [
            "Yongqiang Huang",
            "Yu Sun"
        ],
        "Date": "2016-07-02T00:58:57Z",
        "DOI": [],
        "Category": [
            "cs.RO"
        ],
        "Link": "http://arxiv.org/pdf/1607.00442v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "MPII Dataset",
            "CMU-MMAC Dataset",
            "MPII Cooking Dataset",
            "YouCook Dataset",
            "Rochester ADL Dataset",
            "CAD-120 Dataset",
            "MPII Cooking Composite Dataset",
            "Cooking Eggs Dataset",
            "Gaze+ Dataset",
            "TUM Kitchten Dataset",
            "Salad Dataset",
            "Gaze Dataset",
            "OPPORTUNITY Dataset",
            "ADL Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://rpal.cse.usf.edu/motiondatasetreview/index.htm",
            "http://people.csail.mit.edu/hpirsiav/codes/",
            "http://www.murase.m.is.nagoya-u.ac.jp/KSCGR/",
            "http://robocoffee.org/datasets/",
            "https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/human-activity-recognition/mpii-cooking-2-dataset/",
            "http://openlab.ncl.ac.uk/publicweb/publicweb",
            "https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/human-activity-recognition/mpii-cooking-composite-activities/",
            "http://pr.cs.cornell.edu/humanactivities/",
            "http://ai.stanford.edu/~alireza",
            "http://kitchen.cs.cmu.edu/",
            "http://www.opportunity-project.eu/data.php",
            "http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/",
            "http://www.cs.rochester.edu/~rmessing/uradl/",
            "http://www.cse.buffalo.edu/~jcorso/r/youcook/",
            "https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/human-activity-recognition/mpii-cooking-activities-dataset/",
            "https://archive.ics.uci.edu/ml/datasets/",
            "http://www.eng.yale.edu/grablab/humangrasping/[2][3",
            "https://archive.ics.uci.edu/ml/datasets",
            "https://ias.in.tum.de/software/kitchen-activity-data",
            "http://www.murase.m.is.nagoya-u.ac.jp",
            "http://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/"
        ]
    },
    {
        "Title": "Transforming Question Answering Datasets Into Natural Language Inference  Datasets",
        "Abstract": "Existing datasets for natural language inference (NLI) have propelled research on language understanding. We propose a new method for automatically deriving NLI datasets from the growing abundance of large-scale question answering datasets. Our approach hinges on learning a sentence transformation model which converts question-answer pairs into their declarative forms. Despite being primarily trained on a single QA dataset, we show that it can be successfully applied to a variety of other QA resources. Using this system, we automatically derive a new freely available dataset of over 500k NLI examples (QA-NLI), and show that it exhibits a wide range of inference phenomena rarely seen in previous NLI datasets.",
        "Authors": [
            "Dorottya Demszky",
            "Kelvin Guu",
            "Percy Liang"
        ],
        "Date": "2018-09-09T05:03:34Z",
        "DOI": [],
        "Category": [
            "cs.CL"
        ],
        "Link": "http://arxiv.org/pdf/1809.02922v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Numerous Dataset",
            "QA Dataset",
            "Discussion NLI Dataset",
            "Abstract Existing Dataset",
            "NLI Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://bit.ly/2OMm4vK"
        ]
    },
    {
        "Title": "Kannada-MNIST: A new handwritten digits dataset for the Kannada language",
        "Abstract": "In this paper, we disseminate a new handwritten digits-dataset, termed Kannada-MNIST, for the Kannada script, that can potentially serve as a direct drop-in replacement for the original MNIST dataset. In addition to this dataset, we disseminate an additional real world handwritten dataset (with $10k$ images), which we term as the Dig-MNIST dataset that can serve as an out-of-domain test dataset. We also duly open source all the code as well as the raw scanned images along with the scanner settings so that researchers who want to try out different signal processing pipelines can perform end-to-end comparisons. We provide high level morphological comparisons with the MNIST dataset and provide baselines accuracies for the dataset disseminated. The initial baselines obtained using an oft-used CNN architecture ($96.8\\%$ for the main test-set and $76.1\\%$ for the Dig-MNIST test-set) indicate that these datasets do provide a sterner challenge with regards to generalizability than MNIST or the KMNIST datasets. We also hope this dissemination will spur the creation of similar datasets for all the languages that use different symbols for the numeral digits.",
        "Authors": [
            "Vinay Uday Prabhu"
        ],
        "Date": "2019-08-03T22:33:52Z",
        "DOI": [],
        "Category": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "Link": "http://arxiv.org/pdf/1908.01242v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Dig-MNIST Dataset",
            "MNIST-sized Dataset",
            "Kannada MNIST Dataset",
            "Main Dataset",
            "MNIST Dataset",
            "Kannada-MNIST Dataset",
            "Dig-MNSIT Dataset",
            "MNIST-10k-Test Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://unicode.org/charts/PDF/U0C80.pdf, 2019. [Online; accessed 16-Mar-2019].[10",
            "https://github.com/vinayprabhu/Kannada_MNIST\f",
            "https://en.wikipedia.org/wiki/Kannada, 2019. [Online; accessed 16-Mar-2019].[4",
            "https://en.wikipedia.org/wiki/Eighth_Schedule_to_the",
            "https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py6",
            "https://medium.com/@o.kroeger/tensorflow-mnist-and-your-own-handwritten-digits-4d1cd32bbab45\f",
            "https://github.com/vinayprabhu/Kannada_MNIST/blob/master/colab_notebooks/",
            "https://github.com/vinayprabhu/Kannada_MNIST/blob/master/colab_notebooks/5)",
            "https://gist.github.com/mineshpatel1/",
            "http://yann. lecun. com/exdb/mnist, 2:18, 2010.[2",
            "https://bit.ly/32WTxeT4"
        ]
    },
    {
        "Title": "Dataset2Vec: Learning Dataset Meta-Features",
        "Abstract": "Machine learning tasks such as optimizing the hyper-parameters of a model for a new dataset or few-shot learning can be vastly accelerated if they are not done from scratch for every new dataset, but carry over findings from previous runs. Meta-learning makes use of features of a whole dataset such as its number of instances, its number of predictors, the means of the predictors etc., so called meta-features, dataset summary statistics or simply dataset characteristics, which so far have been hand-crafted, often specifically for the task at hand. More recently, unsupervised dataset encoding models based on variational auto-encoders have been successful in learning such characteristics for the special case when all datasets follow the same schema, but not beyond. In this paper we design a novel model, Dataset2Vec, that is able to characterize datasets with a latent feature vector based on batches and thus is able to generalize beyond datasets having the same schema to arbitrary (tabular) datasets. To do so, we employ auxiliary learning tasks on batches of datasets, esp. to distinguish batches from different datasets. We show empirically that the meta-features collected from batches of similar datasets are concentrated within a small area in the latent space, hence preserving similarity. We also show that using the dataset characteristics learned by Dataset2Vec in a state-of-the-art hyper-parameter optimization model outperforms the hand-crafted meta-features that have been used in the hyper-parameter optimization literature so far. As a result, we advance the current state-of-the-art results for hyper-parameter optimization.",
        "Authors": [
            "Hadi S. Jomaa",
            "Josif Grabocka",
            "Lars Schmidt-Thieme"
        ],
        "Date": "2019-05-27T09:11:57Z",
        "DOI": [],
        "Category": [
            "cs.LG",
            "stat.ML"
        ],
        "Link": "http://arxiv.org/pdf/1905.11063v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Another Dataset",
            "Dataset2Vec: Learning Dataset",
            "Toy Meta Dataset",
            "Evaluation Metric UCI Meta Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://arxiv.org/abs/1802.02219.[11",
            "https://doi.org/10.1109/TIT.2017.2762322.[3",
            "https://github.com/hadijomaa/dataset2vec.git2",
            "https://doi.org/10.1016/j.patcog.2017.10.009.[7",
            "http://archive.ics.uci.edu/ml.[9",
            "http://jmlr.org/proceedings/papers/v31/poczos13a.html.[26",
            "https://doi.org/10.1007/978-3-319-23525-7_7.[36",
            "https://doi.org/10.1007/s10994-017-5684-y.[38",
            "https://github.com/wistuba/TST.git7",
            "http://jmlr.org/proceedings/papers/v28/bardenet13.html.[4",
            "http://proceedings.mlr.press/v70/finn17a.html.[13",
            "http://arxiv.org/abs/1807.08479.9\f[20",
            "https://github.com/conormdurkan/neural-statistician.git3",
            "http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.[32",
            "http://www.bioinf.jku.at/people/klambauer/data_py.zip5"
        ]
    },
    {
        "Title": "VizWiz Dataset Browser: A Tool for Visualizing Machine Learning Datasets",
        "Abstract": "We present a visualization tool to exhaustively search and browse through a set of large-scale machine learning datasets. Built on the top of the VizWiz dataset, our dataset browser tool has the potential to support and enable a variety of qualitative and quantitative research, and open new directions for visualizing and researching with multimodal information. The tool is publicly available at https://vizwiz.org/browse.",
        "Authors": [
            "Nilavra Bhattacharya",
            "Danna Gurari"
        ],
        "Date": "2019-12-19T16:18:34Z",
        "DOI": [],
        "Category": [
            "cs.LG",
            "cs.CV",
            "cs.HC"
        ],
        "Link": "http://arxiv.org/pdf/1912.09336v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "For Dataset",
            "VizWiz Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://mariadb.com/kb/en/library/full-text-index-overview2\f2.5",
            "https://vizwiz.org/browse.",
            "https://www.walmart.com. [Online; ac-11",
            "https://www.ebay.com. [Online; accessed[3",
            "https://www.amazon.com"
        ]
    },
    {
        "Title": "RGB-D-based Action Recognition Datasets: A Survey",
        "Abstract": "Human action recognition from RGB-D (Red, Green, Blue and Depth) data has attracted increasing attention since the first work reported in 2010. Over this period, many benchmark datasets have been created to facilitate the development and evaluation of new algorithms. This raises the question of which dataset to select and how to use it in providing a fair and objective comparative evaluation against state-of-the-art methods. To address this issue, this paper provides a comprehensive review of the most commonly used action recognition related RGB-D video datasets, including 27 single-view datasets, 10 multi-view datasets, and 7 multi-person datasets. The detailed information and analysis of these datasets is a useful resource in guiding insightful selection of datasets for future research. In addition, the issues with current algorithm evaluation vis-\\'{a}-vis limitations of the available datasets and evaluation protocols are also highlighted; resulting in a number of recommendations for collection of new datasets and use of evaluation protocols.",
        "Authors": [
            "Jing Zhang",
            "Wanqing Li",
            "Philip O. Ogunbona"
        ],
        "Date": "2016-01-21T04:58:04Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1601.05511v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "LIRIS Human Activities Dataset",
            "O\ufb03ce Activity Dataset",
            "DMLSmartActions DMLSmartActions Dataset",
            "RGB-D Dataset",
            "MSRActionPair MSRActionPair Dataset",
            "Human Morning Routine Dataset Human Morning Routine Dataset",
            "Mivia Dataset",
            "ATC42 ATC42 Dataset",
            "NJUST RGB-D Action Dataset",
            "SBU Kinect Interaction Dataset",
            "CAD-60 CAD-60 Dataset",
            "WorkoutSU-10 Dataset",
            "RGBD-SAR Dataset",
            "MSRC-12 MSRC-12 Dataset",
            "Such Dataset",
            "UPCV Dataset",
            "CAD-120 CAD-120 Dataset",
            "UTKinect UTKinect Dataset",
            "Human-Object Interaction Dataset",
            "Falling Event Detection Dataset",
            "Osaka University Kinect Action Dataset",
            "CAD-120 Dataset",
            "Falling Detection Dataset",
            "Event Dataset",
            "MSRDailyActivity3D Dataset",
            "Only LIRIS Dataset",
            "UCFKinect Dataset",
            "ReadingAct ReadingAct Dataset",
            "MSR-Action3D Dataset",
            "Human Morning Routine Dataset",
            "UWA3D Multiview UWA3D Multiview Activity Dataset",
            "RGB-D-based Dataset",
            "TJU Dataset",
            "Muti-View TJU Dataset",
            "IAS-lab Action IAS-lab Action Dataset",
            "Mivia Dataset Mivia Dataset",
            "ShakeFive Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://research.microsoft.com/en-_us/um/cambridge/projects/msrc12/)wa",
            "http://www.projects.science.uu.nl/shakefive/), collected by Universiteit",
            "http://adsc.illinois.edu/sites/default/files/files/ADSC-_RGBD-_dataset-_-download-_instructions.pd",
            "http://www.upcv.upatras.gr/personal/kastaniotis/datasets.html)wa",
            "http://vision.sysu.edu.cn/projects/3d-_activity/) was collected by",
            "http://vlm1.uta.edu/~zhangzhong/fall_detection",
            "http://staffhome.ecm.uwa.edu.au/~00053650/databases.htm",
            "http://pr.cs.cornell.edu/humanactivities/data.php) was captured by Cor-nel",
            "http://robotics.dei.unipd.it/actions/index.php/overview) wascollecte",
            "http://www.am.sanken.osaka-_u.ac.jp/~mansur/dataset.htm",
            "http://www.lmars.whu.edu.cn:8086/prof_web/zhuxinyan/DataSetPublish/dataset.htm",
            "http://dipersec.king.ac.uk/G3D/) is a human interaction dataset for multiplayer gamingscenario",
            "http://media-_lab.engr.ccny.cuny.edu/~zcy",
            "http://dml.ece.ubc.ca/data/smartaction/) was collected by the Uni-versit",
            "http://liris.cnrs.fr/voir/activities-_dataset/), collectedb",
            "http://sist.sysu.edu.cn/~zhwshi/students/jianfang",
            "http://media.tju.edu.cn/tju_dataset.html) was captured by Tian-ji",
            "http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.htm",
            "http://dipersec.king.ac.uk/G3D/) captured by Kingston Univer-sit",
            "http://humansensing.cs.cmu.edu/mad/download.html) was created by Carnegie Mellon Uni-versit",
            "http://vipl.ict.ac.cn/rgbd-_action-_dataset)was collected by Institute of Com-putin",
            "http://www.uestcrobot.net/en/?q=download), created by the University of",
            "http://mclab.citi.sinica.edu.tw/dataset/dha/dha.htm",
            "http://mivia.unisa.it/datasets/video-_analysis-_datasets/mivia-_action-_-dataset",
            "http://vpa.sabanciuniv.edu/databases/WorkoutSU-_10/) was collectedb",
            "http://media.tju.edu.cn/tju_dataset.html) was captured by Tianjin Univer-sit",
            "http://www.utdallas.edu/~cxc123730",
            "http://imag.njust.edu.cn/imag/NJUST_RGB-_D_Action_Dataset.htm",
            "http://www.stat.ucla.edu/~ping.wei/research/project",
            "http://www.uni-_tuebingen.de/fakultaeten/mathematisch-_-naturwissenschaftliche-_fakultaet/fachbereiche/informatik/lehrstuehle/human-_computer-_interaction/home/code-_datasets/morning-_routine-_dataset.htm",
            "http://users.eecs.northwestern.edu/~jwa368/my_data.htm",
            "http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html) was collected byth",
            "http://web.ing.puc.cl/~ialillo",
            "https://sites.google.com/site/skicyyu/rgbd_recognition) was col-lecte",
            "http://tele-_immersion.citris-_uc.org/berkeley_mhad#d",
            "http://watchnpatch.cs.cornell.edu/) was collected by Cornell Uni-versit",
            "http://www.cs.ucf.edu/~oreifej",
            "http://pr.cs.cornell.edu/humanactivities/data.php), collected by the Cor-nel",
            "http://research.microsoft.com/en-_us/um/people/zliu/ActionRecoRsrc/) isth"
        ]
    },
    {
        "Title": "Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets",
        "Abstract": "Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks. While model performance on these challenge datasets is significantly lower compared to the original benchmark, it is unclear what particular weaknesses they reveal. For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model's specific training set. We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt. We apply our method to analyze the NLI \"stress tests\" (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang, 2017). We show that after slight exposure, some of these datasets are no longer challenging, while others remain difficult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.",
        "Authors": [
            "Nelson F. Liu",
            "Roy Schwartz",
            "Noah A. Smith"
        ],
        "Date": "2019-04-04T17:04:30Z",
        "DOI": [],
        "Category": [
            "cs.CL"
        ],
        "Link": "http://arxiv.org/pdf/1904.02668v4.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Numerical Reasoning Dataset",
            "Adversarial SQuAD Dataset",
            "NLI Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://nelsonliu.me/papers/inoculation-by-finetuning"
        ]
    },
    {
        "Title": "REPAIR: Removing Representation Bias by Dataset Resampling",
        "Abstract": "Modern machine learning datasets can have biases for certain representations that are leveraged by algorithms to achieve high performance without learning to solve the underlying task. This problem is referred to as \"representation bias\". The question of how to reduce the representation biases of a dataset is investigated and a new dataset REPresentAtion bIas Removal (REPAIR) procedure is proposed. This formulates bias minimization as an optimization problem, seeking a weight distribution that penalizes examples easy for a classifier built on a given feature representation. Bias reduction is then equated to maximizing the ratio between the classification loss on the reweighted dataset and the uncertainty of the ground-truth class labels. This is a minimax problem that REPAIR solves by alternatingly updating classifier parameters and dataset resampling weights, using stochastic gradient descent. An experimental set-up is also introduced to measure the bias of any dataset for a given representation, and the impact of this bias on the performance of recognition models. Experiments with synthetic and action recognition data show that dataset REPAIR can significantly reduce representation bias, and lead to improved generalization of models trained on REPAIRed datasets. The tools used for characterizing representation bias, and the proposed dataset REPAIR algorithm, are available at https://github.com/JerryYLi/Dataset-REPAIR/.",
        "Authors": [
            "Yi Li",
            "Nuno Vasconcelos"
        ],
        "Date": "2019-04-16T18:35:40Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1904.07911v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Minimum-bias Dataset",
            "While Dataset",
            "REPAIRed Dataset",
            "Resampled Dataset",
            "MNIST Dataset",
            "REPAIRed Kinetics Dataset",
            "SGD-based Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://github.com/"
        ]
    },
    {
        "Title": "MultiQA: An Empirical Investigation of Generalization and Transfer in  Reading Comprehension",
        "Abstract": "A large number of reading comprehension (RC) datasets has been created recently, but little analysis has been done on whether they generalize to one another, and the extent to which existing datasets can be leveraged for improving performance on new ones. In this paper, we conduct such an investigation over ten RC datasets, training on one or more source RC datasets, and evaluating generalization, as well as transfer to a target RC dataset. We analyze the factors that contribute to generalization, and show that training on a source RC dataset and transferring to a target dataset substantially improves performance, even in the presence of powerful contextual representations from BERT (Devlin et al., 2019). We also find that training on multiple source RC datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new RC dataset. Following our analysis, we propose MultiQA, a BERT-based model, trained on multiple RC datasets, which leads to state-of-the-art performance on five RC datasets. We share our infrastructure for the benefit of the research community.",
        "Authors": [
            "Alon Talmor",
            "Jonathan Berant"
        ],
        "Date": "2019-05-31T08:05:31Z",
        "DOI": [],
        "Category": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "Link": "http://arxiv.org/pdf/1905.13453v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Each Dataset",
            "Which Dataset",
            "Q&A Dataset",
            "Large Dataset",
            "MULTIQA Test Dataset",
            "Size Dataset",
            "RC Dataset",
            "MULTI-375K Dataset"
        ],
        "Possible_Dataset_Links": []
    },
    {
        "Title": "The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford  RobotCar Dataset",
        "Abstract": "In this paper we present The Oxford Radar RobotCar Dataset, a new dataset for researching scene understanding using Millimetre-Wave FMCW scanning radar data. The target application is autonomous vehicles where this modality remains unencumbered by environmental conditions such as fog, rain, snow, or lens flare, which typically challenge other sensor modalities such as vision and LIDAR.   The data were gathered in January 2019 over thirty-two traversals of a central Oxford route spanning a total of 280 km of urban driving. It encompasses a variety of weather, traffic, and lighting conditions. This 4.7 TB dataset consists of over 240,000 scans from a Navtech CTS350-X radar and 2.4 million scans from two Velodyne HDL-32E 3D LIDARs; along with six cameras, two 2D LIDARs, and a GPS/INS receiver. In addition we release ground truth optimised radar odometry to provide an additional impetus to research in this domain. The full dataset is available for download at: ori.ox.ac.uk/datasets/radar-robotcar-dataset",
        "Authors": [
            "Dan Barnes",
            "Matthew Gadd",
            "Paul Murcutt"
        ],
        "Date": "2019-09-03T16:44:30Z",
        "DOI": [],
        "Category": [
            "cs.RO",
            "eess.SP"
        ],
        "Link": "http://arxiv.org/pdf/1909.01300v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Marulan Dataset",
            "TB Dataset",
            "Oxford RobotCar Dataset",
            "Oxford Radar RobotCar Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://www.openstreetmap.org, 2017.[17",
            "https://www.w3.org/TR/PNG/\fan",
            "https://level5.lyft.com/dataset/, 2019.[9",
            "https://planet.osm.org ,",
            "http://ceres-solver.[21"
        ]
    },
    {
        "Title": "Measuring Dataset Granularity",
        "Abstract": "Despite the increasing visibility of fine-grained recognition in our field, \"fine-grained'' has thus far lacked a precise definition. In this work, building upon clustering theory, we pursue a framework for measuring dataset granularity. We argue that dataset granularity should depend not only on the data samples and their labels, but also on the distance function we choose. We propose an axiomatic framework to capture desired properties for a dataset granularity measure and provide examples of measures that satisfy these properties. We assess each measure via experiments on datasets with hierarchical labels of varying granularity. When measuring granularity in commonly used datasets with our measure, we find that certain datasets that are widely considered fine-grained in fact contain subsets of considerable size that are substantially more coarse-grained than datasets generally regarded as coarse-grained. We also investigate the interplay between dataset granularity with a variety of factors and find that fine-grained datasets are more difficult to learn from, more difficult to transfer to, more difficult to perform few-shot learning with, and more vulnerable to adversarial attacks.",
        "Authors": [
            "Yin Cui",
            "Zeqi Gu",
            "Dhruv Mahajan"
        ],
        "Date": "2019-12-21T00:44:52Z",
        "DOI": [],
        "Category": [
            "cs.CV",
            "cs.LG"
        ],
        "Link": "http://arxiv.org/pdf/1912.10154v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "CUB-200 Dataset",
            "Novel Dataset",
            "Each Dataset",
            "CIFAR Dataset",
            "CIFAR-100 Dataset",
            "Measuring Dataset",
            "Suppose Dataset",
            "CIFAR-10 Dataset",
            "As Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://github.com/richardaecn/dataset-granularity.5.1"
        ]
    },
    {
        "Title": "Reader-Aware Multi-Document Summarization: An Enhanced Model and The  First Dataset",
        "Abstract": "We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online.",
        "Authors": [
            "Piji Li",
            "Lidong Bing",
            "Wai Lam"
        ],
        "Date": "2017-08-03T09:18:16Z",
        "DOI": [],
        "Category": [
            "cs.CL",
            "cs.AI"
        ],
        "Link": "http://arxiv.org/pdf/1708.01065v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Existing Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://tac.nist.gov/knowledge",
            "https://tac.nist.gov/document,",
            "https://goo.gl/DdU0vL",
            "https://en.wikipedia.org/wiki/Category:2017 earthquakes\f3.",
            "http://lpsolve.sourceforge.net/5.5/",
            "http://www.se.cuhk.edu.hk/~textmine/dataset/ra-mds/",
            "http://duc.nist.gov/4"
        ]
    },
    {
        "Title": "DRCD: a Chinese Machine Reading Comprehension Dataset",
        "Abstract": "In this paper, we introduce DRCD (Delta Reading Comprehension Dataset), an open domain traditional Chinese machine reading comprehension (MRC) dataset. This dataset aimed to be a standard Chinese machine reading comprehension dataset, which can be a source dataset in transfer learning. The dataset contains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions generated by annotators. We build a baseline model that achieves an F1 score of 89.59%. F1 score of Human performance is 93.30%.",
        "Authors": [
            "Chih Chieh Shao",
            "Trois Liu",
            "Yuting Lai"
        ],
        "Date": "2018-06-04T01:50:21Z",
        "DOI": [],
        "Category": [
            "cs.CL"
        ],
        "Link": "http://arxiv.org/pdf/1806.00920v3.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "MRC Dataset",
            "DRC Dataset",
            "MSMARCO Dataset",
            "Human Generated Machine Reading Comprehension Dataset",
            "SQuAD Dataset",
            "Chinese Machine Reading Comprehension Dataset",
            "Multiple-choice Dataset",
            "Chinese MRC Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://github.com/EternalFeather/Word2Vec-on-Wikipedia-",
            "https://github.com/fxsjy/jieba ",
            "https://github.com/DRCKnowledgeTeam/",
            "https://github.com/YerevaNN/R-NET-in-Keras ",
            "https://github.com/google-research/bert \fsupervise",
            "https://github.com/NLPLearn/QANet ",
            "http://aclweb.org/anthology/C16-1167",
            "https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12",
            "https://github.com/BYVoid/OpenCC an"
        ]
    },
    {
        "Title": "Synthetic Human Model Dataset for Skeleton Driven Non-rigid Motion  Tracking and 3D Reconstruction",
        "Abstract": "We introduce a synthetic dataset for evaluating non-rigid 3D human reconstruction based on conventional RGB-D cameras. The dataset consist of seven motion sequences of a single human model. For each motion sequence per-frame ground truth geometry and ground truth skeleton are given. The dataset also contains skinning weights of the human model. More information about the dataset can be found at: https://research.csiro.au/robotics/our-work/databases/synthetic-human-model-dataset/",
        "Authors": [
            "Shafeeq Elanattil",
            "Peyman Moghadam"
        ],
        "Date": "2019-03-07T01:13:24Z",
        "DOI": "10.25919/5c495488b0f4e",
        "Category": [
            "cs.CV",
            "I.2.10; I.4; I.5"
        ],
        "Link": "http://arxiv.org/pdf/1903.02679v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Synthetic Human Model Dataset",
            "CMU Mocap Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://doi.org/10.25919/5b7b60176d0cd[15",
            "https://research.csiro.au/robotics/our-work/databases/synthetic-human-model-dataset/foundat:be"
        ]
    },
    {
        "Title": "A synthetic dataset for deep learning",
        "Abstract": "In this paper, we propose a novel method for generating a synthetic dataset obeying Gaussian distribution. Compared to the commonly used benchmark datasets with unknown distribution, the synthetic dataset has an explicit distribution, i.e., Gaussian distribution. Meanwhile, it has the same characteristics as the benchmark dataset MNIST. As a result, we can easily apply Deep Neural Networks (DNNs) on the synthetic dataset. This synthetic dataset provides a novel experimental tool to verify the proposed theories of deep learning.",
        "Authors": [
            "Xinjie Lan"
        ],
        "Date": "2019-06-01T05:16:40Z",
        "DOI": [],
        "Category": [
            "cs.CV",
            "stat.AP",
            "stat.ML"
        ],
        "Link": "http://arxiv.org/pdf/1906.11905v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "NIST Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://www.nist.gov/srd/nist-special-database-19"
        ]
    },
    {
        "Title": "Towards More Usable Dataset Search: From Query Characterization to  Snippet Generation",
        "Abstract": "Reusing published datasets on the Web is of great interest to researchers and developers. Their data needs may be met by submitting queries to a dataset search engine to retrieve relevant datasets. In this ongoing work towards developing a more usable dataset search engine, we characterize real data needs by annotating the semantics of 1,947 queries using a novel fine-grained scheme, to provide implications for enhancing dataset search. Based on the findings, we present a query-centered framework for dataset search, and explore the implementation of snippet generation and evaluate it with a preliminary user study.",
        "Authors": [
            "Jinchi Chen",
            "Xiaxia Wang",
            "Gong Cheng"
        ],
        "Date": "2019-08-29T10:48:26Z",
        "DOI": "10.1145/3357384.3358096",
        "Category": [
            "cs.IR"
        ],
        "Link": "http://arxiv.org/pdf/1908.11146v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "DING! Dataset",
            "Google Dataset",
            "RDF Dataset",
            "KEYWORDS Dataset",
            "Towards More Usable Dataset",
            "HUST-ASL Dataset",
            "FDA Dataset",
            "Hierarchical Dataset",
            "Characterising Dataset",
            "RELATED WORK Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://doi.org/10.1109/ICSC.2013.12[7",
            "http://ws.nju.edu.cn/datasetsearch/query-cikm2019/dataneeds.txt3",
            "https://opendata.stackexchange.com/questions/11146/dataset-for-road-accidents-or-traffic2",
            "https://doi.org/10.1109",
            "https://doi.org/10.1007/978-3-030-00668-6_9[10",
            "http://ws.nju.edu.cn/datasetsearch/query-cikm2019/annotations.txt2",
            "https://doi.org/10.1016/j.websem.2018.11.003[5",
            "https://doi.org/10.1145/2882903.2915217[8",
            "https://doi.org/10.1109/ICDE.2007.367929[4",
            "http://ws.nju.edu.cn/datasetsearch/query-cikm2019/queries.txt4",
            "https://doi.org/10.1145/3308558.3313685[9"
        ]
    },
    {
        "Title": "Clustering Mixed Numeric and Categorical Data: A Cluster Ensemble  Approach",
        "Abstract": "Clustering is a widely used technique in data mining applications for discovering patterns in underlying data. Most traditional clustering algorithms are limited to handling datasets that contain either numeric or categorical attributes. However, datasets with mixed types of attributes are common in real life data mining applications. In this paper, we propose a novel divide-and-conquer technique to solve this problem. First, the original mixed dataset is divided into two sub-datasets: the pure categorical dataset and the pure numeric dataset. Next, existing well established clustering algorithms designed for different types of datasets are employed to produce corresponding clusters. Last, the clustering results on the categorical and numeric dataset are combined as a categorical dataset, on which the categorical data clustering algorithm is used to get the final clusters. Our contribution in this paper is to provide an algorithm framework for the mixed attributes clustering problem, in which existing clustering algorithms can be easily integrated, the capabilities of different kinds of clustering algorithms and characteristics of different types of datasets could be fully exploited. Comparisons with other clustering algorithms on real life datasets illustrate the superiority of our approach.",
        "Authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "Date": "2005-09-05T02:47:12Z",
        "DOI": [],
        "Category": [
            "cs.AI"
        ],
        "Link": "http://arxiv.org/pdf/cs/0509011v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [],
        "Possible_Dataset_Links": []
    },
    {
        "Title": "EMBER: An Open Dataset for Training Static PE Malware Machine Learning  Models",
        "Abstract": "This paper describes EMBER: a labeled benchmark dataset for training machine learning models to statically detect malicious Windows portable executable files. The dataset includes features extracted from 1.1M binary files: 900K training samples (300K malicious, 300K benign, 300K unlabeled) and 200K test samples (100K malicious, 100K benign). To accompany the dataset, we also release open source code for extracting features from additional binaries so that additional sample features can be appended to the dataset. This dataset fills a void in the information security machine learning community: a benign/malicious dataset that is large, open and general enough to cover several interesting use cases. We enumerate several use cases that we considered when structuring the dataset. Additionally, we demonstrate one use case wherein we compare a baseline gradient boosted decision tree model trained using LightGBM with default settings to MalConv, a recently published end-to-end (featureless) deep learning model for malware detection. Results show that even without hyper-parameter optimization, the baseline EMBER model outperforms MalConv. The authors hope that the dataset, code and baseline model provided by EMBER will help invigorate machine learning research for malware detection, in much the same way that benchmark datasets have advanced computer vision research.",
        "Authors": [
            "Hyrum S. Anderson",
            "Phil Roth"
        ],
        "Date": "2018-04-12T17:23:56Z",
        "DOI": [],
        "Category": [
            "cs.CR"
        ],
        "Link": "http://arxiv.org/pdf/1804.04637v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "EMBER: An Open Dataset",
            "EMBER Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://www. nexginrc.org/papers/tr21-zubair",
            "http://83.133.184.251/virensimulation.org/index.html",
            "https://github.com/endgameinc/ember.",
            "https://sourceforge.net/adobe/malclassifier/wiki/Home/\ftime.tection.",
            "https://upload.wikimedia.org/wikipedia/commons/1/1b/",
            "https:// (case insensitive) that may indicate a",
            "http:// or ",
            "https://virusshare.com. Accessed: 2018-03-[25",
            "https://www.virustotal.com/en",
            "https://github.com/endgameinc/ember\fsourc"
        ]
    },
    {
        "Title": "Cross-Dataset Adaptation for Visual Question Answering",
        "Abstract": "We investigate the problem of cross-dataset adaptation for visual question answering (Visual QA). Our goal is to train a Visual QA model on a source dataset but apply it to another target one. Analogous to domain adaptation for visual recognition, this setting is appealing when the target dataset does not have a sufficient amount of labeled data to learn an \"in-domain\" model. The key challenge is that the two datasets are constructed differently, resulting in the cross-dataset mismatch on images, questions, or answers.   We overcome this difficulty by proposing a novel domain adaptation algorithm. Our method reduces the difference in statistical distributions by transforming the feature representation of the data in the target dataset. Moreover, it maximizes the likelihood of answering questions (in the target dataset) correctly using the Visual QA model trained on the source dataset. We empirically studied the effectiveness of the proposed approach on adapting among several popular Visual QA datasets. We show that the proposed method improves over baselines where there is no adaptation and several other adaptation methods. We both quantitatively and qualitatively analyze when the adaptation can be mostly effective.",
        "Authors": [
            "Wei-Lun Chao",
            "Hexiang Hu",
            "Fei Sha"
        ],
        "Date": "2018-06-10T21:06:28Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1806.03726v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "QA Dataset",
            "Visual QA Dataset",
            "Cross Dataset",
            "VQA Dataset"
        ],
        "Possible_Dataset_Links": []
    },
    {
        "Title": "Characterizing multiple instance datasets",
        "Abstract": "In many pattern recognition problems, a single feature vector is not sufficient to describe an object. In multiple instance learning (MIL), objects are represented by sets (\\emph{bags}) of feature vectors (\\emph{instances}). This requires an adaptation of standard supervised classifiers in order to train and evaluate on these bags of instances. Like for supervised classification, several benchmark datasets and numerous classifiers are available for MIL. When performing a comparison of different MIL classifiers, it is important to understand the differences of the datasets, used in the comparison. Seemingly different (based on factors such as dimensionality) datasets may elicit very similar behaviour in classifiers, and vice versa. This has implications for what kind of conclusions may be drawn from the comparison results. We aim to give an overview of the variability of available benchmark datasets and some popular MIL classifiers. We use a dataset dissimilarity measure, based on the differences between the ROC-curves obtained by different classifiers, and embed this dataset dissimilarity matrix into a low-dimensional space. Our results show that conceptually similar datasets can behave very differently. We therefore recommend examining such dataset characteristics when making comparisons between existing and new MIL classifiers.   The datasets are available via Figshare at \\url{https://bit.ly/2K9iTja}.",
        "Authors": [
            "Veronika Cheplygina",
            "David M. J. Tax"
        ],
        "Date": "2018-06-21T11:54:49Z",
        "DOI": "10.1007/978-3-319-24261-3_2",
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1806.08186v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Arti\ufb01cial Dataset",
            "MIL Dataset",
            "Real-life Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://homepage.tudelft.nl/n9d04/milweb/ .",
            "https://bit.ly/2K9iTja.1"
        ]
    },
    {
        "Title": "Data collaboration analysis for distributed datasets",
        "Abstract": "In this paper, we propose a data collaboration analysis method for distributed datasets. The proposed method is a centralized machine learning while training datasets and models remain distributed over some institutions. Recently, data became large and distributed with decreasing costs of data collection. If we can centralize these distributed datasets and analyse them as one dataset, we expect to obtain novel insight and achieve a higher prediction performance compared with individual analyses on each distributed dataset. However, it is generally difficult to centralize the original datasets due to their huge data size or regarding a privacy-preserving problem. To avoid these difficulties, we propose a data collaboration analysis method for distributed datasets without sharing the original datasets. The proposed method centralizes only intermediate representation constructed individually instead of the original dataset.",
        "Authors": [
            "Akira Imakura",
            "Tetsuya Sakurai"
        ],
        "Date": "2019-02-20T12:33:39Z",
        "DOI": [],
        "Category": [
            "cs.LG",
            "stat.ML"
        ],
        "Link": "http://arxiv.org/pdf/1902.07535v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [],
        "Possible_Dataset_Links": [
            "https://arxiv.org/abs/1610.05492[17",
            "https://aaai.org/Conferences/AAAI-19/invited-speakers/7"
        ]
    },
    {
        "Title": "A New Benchmark Dataset for Texture Image Analysis and Surface Defect  Detection",
        "Abstract": "Texture analysis plays an important role in many image processing applications to describe the image content or objects. On the other hand, visual surface defect detection is a highly research field in the computer vision. Surface defect refers to abnormalities in the texture of the surface. So, in this paper a dual purpose benchmark dataset is proposed for texture image analysis and surface defect detection titled stone texture image (STI dataset). The proposed benchmark dataset consist of 4 different class of stone texture images. The proposed benchmark dataset have some unique properties to make it very near to real applications. Local rotation, different zoom rates, unbalanced classes, variation of textures in size are some properties of the proposed dataset. In the result part, some descriptors are applied on this dataset to evaluate the proposed STI dataset in comparison with other state-of-the-art datasets.",
        "Authors": [
            "Shervan Fekri-Ershad"
        ],
        "Date": "2019-06-27T11:36:29Z",
        "DOI": "10.13140/RG.2.2.33612.46722",
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1906.11561v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Outex Dataset",
            "Brodatz Dataset",
            "Trunk12 Dataset",
            "Each Dataset",
            "Outex Tc-000030 Dataset",
            "New Benchmark Dataset",
            "Stone Texture Dataset",
            "VisTex Dataset",
            "Proposed Dataset",
            "STI Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://www.outex.oulu.fi/). [8",
            "http://vismod.media.mit.edu/pub/ ",
            "https://www.researchgate.net/publication/333755130_STI_Dataset_Link [13"
        ]
    },
    {
        "Title": "ToyADMOS: A Dataset of Miniature-Machine Operating Sounds for Anomalous  Sound Detection",
        "Abstract": "This paper introduces a new dataset called \"ToyADMOS\" designed for anomaly detection in machine operating sounds (ADMOS). To the best our knowledge, no large-scale datasets are available for ADMOS, although large-scale datasets have contributed to recent advancements in acoustic signal processing. This is because anomalous sound data are difficult to collect. To build a large-scale dataset for ADMOS, we collected anomalous operating sounds of miniature machines (toys) by deliberately damaging them. The released dataset consists of three sub-datasets for machine-condition inspection, fault diagnosis of machines with geometrically fixed tasks, and fault diagnosis of machines with moving tasks. Each sub-dataset includes over 180 hours of normal machine-operating sounds and over 4,000 samples of anomalous sounds collected with four microphones at a 48-kHz sampling rate. The dataset is freely available for download at https://github.com/YumaKoizumi/ToyADMOS-dataset",
        "Authors": [
            "Yuma Koizumi",
            "Shoichiro Saito",
            "Hisashi Uematsu"
        ],
        "Date": "2019-08-09T03:52:08Z",
        "DOI": [],
        "Category": [
            "eess.AS",
            "cs.LG",
            "cs.SD",
            "stat.ML"
        ],
        "Link": "http://arxiv.org/pdf/1908.03299v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "ToyADMOS Dataset",
            "Large-scale Dataset",
            "Freesound Dataset",
            "CNT Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://github.com",
            "https://github.com/YumaKoizumi/",
            "http://dx.doi.org/10.7488/ds/1994"
        ]
    },
    {
        "Title": "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age",
        "Abstract": "Existing public face datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. This can lead to inconsistent model accuracy, limit the applicability of face analytic systems to non-White race groups, and adversely affect research findings based on such skewed data. To mitigate the race bias in these datasets, we construct a novel face image dataset, containing 108,501 images, with an emphasis of balanced race composition in the dataset. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle East, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent between race and gender groups.",
        "Authors": [
            "Kimmo K\u00e4rkk\u00e4inen",
            "Jungseock Joo"
        ],
        "Date": "2019-08-14T01:42:41Z",
        "DOI": [],
        "Category": [
            "cs.CV",
            "cs.LG"
        ],
        "Link": "http://arxiv.org/pdf/1908.04913v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "FairFace: Face Attribute Dataset",
            "Yahoo YFCC100M Dataset",
            "LFWA+ Dataset",
            "FaceScrub Dataset",
            "YFCC-100M Flickr Dataset",
            "Protest Dataset",
            "YFCC100M Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://github.com/joojs/fairface}.1"
        ]
    },
    {
        "Title": "Soft-Label Dataset Distillation and Text Dataset Distillation",
        "Abstract": "Dataset distillation is a method for reducing dataset sizes by learning a small number of synthetic samples containing all the information of a large dataset. This has several benefits like speeding up model training, reducing energy consumption, and reducing required storage space. Currently, each synthetic sample is assigned a single `hard' label, and also, dataset distillation can currently only be used with image data.   We propose to simultaneously distill both images and their labels, thus assigning each synthetic sample a `soft' label (a distribution of labels). Our algorithm increases accuracy by 2-4% over the original algorithm for several image classification tasks. Using `soft' labels also enables distilled datasets to consist of fewer samples than there are classes as each sample can encode information for multiple classes. For example, training a LeNet model with 10 distilled images (one per class) results in over 96% accuracy on MNIST, and almost 92% accuracy when trained on just 5 distilled images.   We also extend the dataset distillation algorithm to distill sequential datasets including texts. We demonstrate that text distillation outperforms other methods across multiple datasets. For example, models attain almost their original accuracy on the IMDB sentiment analysis task using just 20 distilled sentences.",
        "Authors": [
            "Ilia Sucholutsky",
            "Matthias Schonlau"
        ],
        "Date": "2019-10-06T23:57:22Z",
        "DOI": [],
        "Category": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "Link": "http://arxiv.org/pdf/1910.02551v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Keywords: Dataset",
            "IMDB Dataset",
            "Abstract Dataset",
            "Soft-Label Dataset",
            "Knowledge Distillation Dataset",
            "With Dataset",
            "Improving Dataset",
            "Text Dataset",
            "Baselines Dataset",
            "Similarly, Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://www.cs.toronto.edu/~kriz/learning-features-2009"
        ]
    },
    {
        "Title": "A Semi-Automatic Approach for Detecting Dataset References in Social  Science Texts",
        "Abstract": "Today, full-texts of scientific articles are often stored in different locations than the used datasets. Dataset registries aim at a closer integration by making datasets citable but authors typically refer to datasets using inconsistent abbreviations and heterogeneous metadata (e.g. title, publication year). It is thus hard to reproduce research results, to access datasets for further analysis, and to determine the impact of a dataset. Manually detecting references to datasets in scientific articles is time-consuming and requires expert knowledge in the underlying research domain.We propose and evaluate a semi-automatic three-step approach for finding explicit references to datasets in social sciences articles.We first extract pre-defined special features from dataset titles in the da|ra registry, then detect references to datasets using the extracted features, and finally match the references found with corresponding dataset titles. The approach does not require a corpus of articles (avoiding the cold start problem) and performs well on a test corpus. We achieved an F-measure of 0.84 for detecting references in full-texts and an F-measure of 0.83 for finding correct matches of detected references in the da|ra dataset registry.",
        "Authors": [
            "Behnam Ghavimi",
            "Philipp Mayr",
            "Christoph Lange"
        ],
        "Date": "2016-11-06T18:36:16Z",
        "DOI": [],
        "Category": [
            "cs.DL"
        ],
        "Link": "http://arxiv.org/pdf/1611.01820v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "However, Dataset",
            "Detecting Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://www.gesis.org/en/services/publications/mda/8",
            "http://www.ssoar.info7",
            "https://github.com/OSCOSS/testdata_mda1",
            "https://github.com/OSCOSS/testdata_mda/blob/master/ELPub_Corpus_evaluation.csv",
            "http://www.da-ra.de9",
            "http://purl.org/net/epubs/work/43640.[8",
            "http://da-ra.de/oaip/\f3",
            "http://dl.acm.org/citation.cfm?id=645328.650004.[15"
        ]
    },
    {
        "Title": "Deep Learning with Lung Segmentation and Bone Shadow Exclusion  Techniques for Chest X-Ray Analysis of Lung Cancer",
        "Abstract": "The recent progress of computing, machine learning, and especially deep learning, for image recognition brings a meaningful effect for automatic detection of various diseases from chest X-ray images (CXRs). Here efficiency of lung segmentation and bone shadow exclusion techniques is demonstrated for analysis of 2D CXRs by deep learning approach to help radiologists identify suspicious lesions and nodules in lung cancer patients. Training and validation was performed on the original JSRT dataset (dataset #01), BSE-JSRT dataset, i.e. the same JSRT dataset, but without clavicle and rib shadows (dataset #02), original JSRT dataset after segmentation (dataset #03), and BSE-JSRT dataset after segmentation (dataset #04). The results demonstrate the high efficiency and usefulness of the considered pre-processing techniques in the simplified configuration even. The pre-processed dataset without bones (dataset #02) demonstrates the much better accuracy and loss results in comparison to the other pre-processed datasets after lung segmentation (datasets #02 and #03).",
        "Authors": [
            "Yu. Gordienko",
            "Peng Gang",
            "Jiang Hui"
        ],
        "Date": "2017-12-20T18:40:49Z",
        "DOI": "10.1007/978-3-319-91008-6_63",
        "Category": [
            "cs.LG",
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1712.07632v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "MC Dataset",
            "JSRT, Dataset",
            "Both Dataset",
            "BSE-JSRT Dataset",
            "JSRT Dataset",
            "ChestX-ray14 Dataset",
            "X-ray Dataset",
            "Shenzhen Hospital Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://comsys.kpi.ua ",
            "https://github.com/imlab-uiip/lung-segmentation-2",
            "http://www.hzu.edu.cn "
        ]
    },
    {
        "Title": "Quick, Stat!: A Statistical Analysis of the Quick, Draw! Dataset",
        "Abstract": "The Quick, Draw! Dataset is a Google dataset with a collection of 50 million drawings, divided in 345 categories, collected from the users of the game Quick, Draw!. In contrast with most of the existing image datasets, in the Quick, Draw! Dataset, drawings are stored as time series of pencil positions instead of a bitmap matrix composed by pixels. This aspect makes this dataset the largest doodle dataset available at the time. The Quick, Draw! Dataset is presented as a great opportunity to researchers for developing and studying machine learning techniques. Due to the size of this dataset and the nature of its source, there is a scarce of information about the quality of the drawings contained. In this paper, a statistical analysis of three of the classes contained in the Quick, Draw! Dataset is depicted: mountain, book and whale. The goal is to give to the reader a first impression of the data collected in this dataset. For the analysis of the quality of the drawings, a Classification Neural Network was trained to obtain a classification score. Using this classification score and the parameters provided by the dataset, a statistical analysis of the quality and nature of the drawings contained in this dataset is provided.",
        "Authors": [
            "Raul Fernandez-Fernandez",
            "Juan G. Victores",
            "David Estevez"
        ],
        "Date": "2019-07-15T10:28:34Z",
        "DOI": "10.11128/arep.58",
        "Category": [
            "cs.CV",
            "cs.DB",
            "eess.IV",
            "68T30",
            "I.2.6; I.5.1; I.3.4"
        ],
        "Link": "http://arxiv.org/pdf/1907.06417v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Google Dataset",
            "Quick, Draw! Dataset",
            "Eitz Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://github.com/googlecreativelab/quickdraw-dataset\f126",
            "https://quickdraw.withgoogle.com/\f2",
            "https://doi.org/10.1007/978-3-030-01234-2_182",
            "https://doi.org/10.1109/CVPR.2009.5206848",
            "https://doi.org/10.1109/TPAMI.2008.22210",
            "https://tinyurl.com/tensorflow-models-quickdraw4864963",
            "https://doi.org/10.1109/CVPR.2015.72985943",
            "http://ndjson.org/\f5"
        ]
    },
    {
        "Title": "EgoNet-UIUC: A Dataset For Ego Network Research",
        "Abstract": "In this report, we introduce the version one of EgoNet-UIUC, which is a dataset for ego network research. The dataset contains about 230 ego networks in Linkedin, which have about 33K users (with their attributes) and 283K relationships (with their relationship types) in total. We name this dataset as EgoNet-UIUC, which stands for Ego Network Dataset from University of Illinois at Urbana-Champaign.",
        "Authors": [
            "Rui Li",
            "Kevin Chen-Chuan Chang"
        ],
        "Date": "2013-09-17T02:28:25Z",
        "DOI": [],
        "Category": [
            "cs.SI",
            "physics.soc-ph"
        ],
        "Link": "http://arxiv.org/pdf/1309.4157v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Ego Network Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://forward.cs.illinois.edu/demos/linkedin/about.html1\fschool:precollework:acquaintance5%work",
            "https://wiki.engr.illinois.edu/display/forward/Dataset-CP-LinkedinCrawl-Aug2013"
        ]
    },
    {
        "Title": "Baselines and a datasheet for the Cerema AWP dataset",
        "Abstract": "This paper presents the recently published Cerema AWP (Adverse Weather Pedestrian) dataset for various machine learning tasks and its exports in machine learning friendly format. We explain why this dataset can be interesting (mainly because it is a greatly controlled and fully annotated image dataset) and present baseline results for various tasks. Moreover, we decided to follow the very recent suggestions of datasheets for dataset, trying to standardize all the available information of the dataset, with a transparency objective.",
        "Authors": [
            "Isma\u00efla Seck",
            "Khouloud Dahmane",
            "Pierre Duthon"
        ],
        "Date": "2018-06-11T14:22:48Z",
        "DOI": [],
        "Category": [
            "cs.LG",
            "stat.ML"
        ],
        "Link": "http://arxiv.org/pdf/1806.04016v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Weather Pedestrian) Dataset",
            "Cerema AWP Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://www.cerema.fr/system/files/documents/technology2017/",
            "https://github.com/hindupuravinash/the-gan-zoo",
            "https://ceremadlcfmds.wixsite.com/cerema-databases",
            "http://opendatacommons.org/licenses/dbcl/1.0/.",
            "http://opendatacommons.org/licenses/odbl/1.0/.",
            "https://github.com/gloosli/cerem"
        ]
    },
    {
        "Title": "How robust is MovieLens? A dataset analysis for recommender systems",
        "Abstract": "Research publication requires public datasets. In recommender systems, some datasets are largely used to compare algorithms against a --supposedly-- common benchmark. Problem: for various reasons, these datasets are heavily preprocessed, making the comparison of results across papers difficult. This paper makes explicit the variety of preprocessing and evaluation protocols to test the robustness of a dataset (or lack of flexibility). While robustness is good to compare results across papers, for flexible datasets we propose a method to select a preprocessing protocol and share results more transparently.",
        "Authors": [
            "Anne-Marie Tousch"
        ],
        "Date": "2019-09-12T09:36:31Z",
        "DOI": [],
        "Category": [
            "cs.IR",
            "cs.LG"
        ],
        "Link": "http://arxiv.org/pdf/1909.12799v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "RecoGym Dataset",
            "MovieLens Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        ]
    },
    {
        "Title": "Clustering on Multiple Incomplete Datasets via Collective Kernel  Learning",
        "Abstract": "Multiple datasets containing different types of features may be available for a given task. For instance, users' profiles can be used to group users for recommendation systems. In addition, a model can also use users' historical behaviors and credit history to group users. Each dataset contains different information and suffices for learning. A number of clustering algorithms on multiple datasets were proposed during the past few years. These algorithms assume that at least one dataset is complete. So far as we know, all the previous methods will not be applicable if there is no complete dataset available. However, in reality, there are many situations where no dataset is complete. As in building a recommendation system, some new users may not have a profile or historical behaviors, while some may not have a credit history. Hence, no available dataset is complete. In order to solve this problem, we propose an approach called Collective Kernel Learning to infer hidden sample similarity from multiple incomplete datasets. The idea is to collectively completes the kernel matrices of incomplete datasets by optimizing the alignment of the shared instances of the datasets. Furthermore, a clustering algorithm is proposed based on the kernel matrix. The experiments on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. The proposed clustering algorithm outperforms the comparison algorithms by as much as two times in normalized mutual information.",
        "Authors": [
            "Weixiang Shao",
            "Xiaoxiao Shi",
            "Philip S. Yu"
        ],
        "Date": "2013-10-04T06:18:59Z",
        "DOI": [],
        "Category": [
            "cs.LG",
            "H.2.8; I.5.3"
        ],
        "Link": "http://arxiv.org/pdf/1310.1177v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Each Dataset",
            "Seeds Dataset",
            "Description Incomplete Dataset",
            "Different Dataset",
            "Abstract\u2014Multiple Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://archive.ics.uci.edu/ml/datasets/seeds[5"
        ]
    },
    {
        "Title": "Photographic dataset: random peppercorns",
        "Abstract": "This is a photographic dataset collected for testing image processing algorithms. The idea is to have sets of different but statistically similar images. In this work the images show randomly distributed peppercorns. The dataset is made available at www.fips.fi/photographic_dataset.php .",
        "Authors": [
            "Teemu Helenius",
            "Samuli Siltanen"
        ],
        "Date": "2016-03-03T10:24:07Z",
        "DOI": [],
        "Category": [
            "physics.data-an",
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1603.01046v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [],
        "Possible_Dataset_Links": []
    },
    {
        "Title": "The HASYv2 dataset",
        "Abstract": "This paper describes the HASYv2 dataset. HASY is a publicly available, free of charge dataset of single symbols similar to MNIST. It contains 168233 instances of 369 classes. HASY contains two challenges: A classification challenge with 10 pre-defined folds for 10-fold cross-validation and a verification challenge.",
        "Authors": [
            "Martin Thoma"
        ],
        "Date": "2017-01-29T13:42:14Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1701.08380v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "HASY Dataset",
            "Complete Dataset",
            "HASYv2 Dataset",
            "HWRT Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://arxiv.org/abs/1412.6980",
            "https://arxiv.org/pdf/1512.03385v1.pdf",
            "https://doi.org/10.5281/i",
            "http://www.matthewzeiler.com/pubs/icml2013/icml2013.pdf",
            "https://www.tensorflow.org/tutorials/mnist/pros/[tf-16b]",
            "http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf",
            "http://write-math.com.",
            "http://danielkirs.ch/thesis.pdf",
            "http://tensorflow.org/\f5",
            "https://arxiv.org/abs/1608.06993v1"
        ]
    },
    {
        "Title": "A Bayesian model for microarray datasets merging",
        "Abstract": "The aggregation of microarray datasets originating from different studies is still a difficult open problem. Currently, best results are generally obtained by the so-called meta-analysis approach, which aggregates results from individual datasets, instead of analyzing aggre-gated datasets. In order to tackle such aggregation problems, it is necessary to correct for interstudy variability prior to aggregation. The goal of this paper is to present a new approach for microarray datasets merging, based upon explicit modeling of interstudy variability and gene variability. We develop and demonstrate a new algorithm for microarray datasets merging. The underlying model assumes normally distributed intrinsic gene expressions, distorted by a study-dependent nonlinear transformation, and study dependent (normally distributed) observation noise. The algorithm addresses both parameter estimation (the parameters being gene expression means and variances, observation noise variances and the nonlinear transformations) and data adjustment, and yields as a result adjusted datasets suitable for aggregation. The method is validated on two case studies. The first one concerns E. Coli expression data, artificially distorted by given nonlinear transformations and additive observation noise. The proposed method is able to correct for the distortion, and yields adjusted datasets from which the relevant biological effects can be recovered, as shown by a standard differential analysis. The second case study concerns the aggregation of two real prostate cancer datasets. After adjustment using the proposed algorithm, a differential analysis performed on adjusted datasets yields a larger number of differentially expressed genes (between control and tumor data). The proposed method has been implemented using the statistical software R 1, and Bioconductor packages 2. The source code (valid for merging two datasets), as well as the datasets used for the validation, and some complementary results, are made available on the web site",
        "Authors": [
            "Marie-Christine Roubaud",
            "Bruno Torr\u00e9sani"
        ],
        "Date": "2015-10-27T10:43:24Z",
        "DOI": [],
        "Category": [
            "stat.ME",
            "q-bio.QM"
        ],
        "Link": "http://arxiv.org/pdf/1510.07850v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Both Dataset",
            "Stuart Dataset",
            "Gray: Dataset",
            "Coli Dataset"
        ],
        "Possible_Dataset_Links": []
    },
    {
        "Title": "Navigating the Data Lake with Datamaran: Automatically Extracting  Structure from Log Datasets",
        "Abstract": "Organizations routinely accumulate semi-structured log datasets generated as the output of code; these datasets remain unused and uninterpreted, and occupy wasted space - this phenomenon has been colloquially referred to as \"data lake\" problem. One approach to leverage these semi-structured datasets is to convert them into a structured relational format, following which they can be analyzed in conjunction with other datasets. We present Datamaran, an tool that extracts structure from semi-structured log datasets with no human supervision. Datamaran automatically identifies field and record endpoints, separates the structured parts from the unstructured noise or formatting, and can tease apart multiple structures from within a dataset, in order to efficiently extract structured relational datasets from semi-structured log datasets, at scale with high accuracy. Compared to other unsupervised log dataset extraction tools developed in prior work, Datamaran does not require the record boundaries to be known beforehand, making it much more applicable to the noisy log files that are ubiquitous in data lakes. Datamaran can successfully extract structured information from all datasets used in prior work, and can achieve 95% extraction accuracy on automatically collected log datasets from GitHub - a substantial 66% increase of accuracy compared to unsupervised schemes from prior work. Our user study further demonstrates that the extraction results of Datamaran are closer to the desired structure than competing algorithms.",
        "Authors": [
            "Yihan Gao",
            "Silu Huang",
            "Aditya Parameswaran"
        ],
        "Date": "2017-08-29T17:47:08Z",
        "DOI": [],
        "Category": [
            "cs.DB"
        ],
        "Link": "http://arxiv.org/pdf/1708.08905v3.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "For Dataset",
            "GitHub Dataset",
            "One Dataset",
            "Single-Line Dataset",
            "Log Dataset",
            "Extracted Relational Dataset",
            "Structure) Description Dataset",
            "Multi-Line Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://doi.org/",
            "https://en.wikipedia.org/wiki/Flex_(lexical_[3",
            "http://www.gartner.com/newsroom/id/2809117, 2014.[47",
            "http://dx.doi.org/10.1145/XXXXXX.XXXXXX22",
            "https://arxiv.org/abs/1708.08905.[2",
            "http://cloudera.[4",
            "https://archive.org/details/stackexchange. Accessed:analyser_generato"
        ]
    },
    {
        "Title": "A Large Dataset for Improving Patch Matching",
        "Abstract": "We propose a new dataset for learning local image descriptors which can be used for significantly improved patch matching. Our proposed dataset consists of an order of magnitude more number of scenes, images, and positive and negative correspondences compared to the currently available Multi-View Stereo (MVS) dataset from Brown et al. The new dataset also has better coverage of the overall viewpoint, scale, and lighting changes in comparison to the MVS dataset. Our dataset also provides supplementary information like RGB patches with scale and rotations values, and intrinsic and extrinsic camera parameters which as shown later can be used to customize training data as per application. We train an existing state-of-the-art model on our dataset and evaluate on publicly available benchmarks such as HPatches dataset and Strecha et al.\\cite{strecha} to quantify the image descriptor performance. Experimental evaluations show that the descriptors trained using our proposed dataset outperform the current state-of-the-art descriptors trained on MVS by 8%, 4% and 10% on matching, verification and retrieval tasks respectively on the HPatches dataset. Similarly on the Strecha dataset, we see an improvement of 3-5% for the matching task in non-planar scenes.",
        "Authors": [
            "Rahul Mitra",
            "Nehal Doiphode",
            "Utkarsh Gautam"
        ],
        "Date": "2018-01-04T17:37:45Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1801.01466v3.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Oxford-Af\ufb01ne Dataset",
            "Features Dataset",
            "Proposed PS Dataset",
            "HPatches Dataset",
            "Large Dataset",
            "Baseline Dataset",
            "Hpatches Dataset",
            "CDVS Dataset",
            "PS Dataset",
            "DTU Dataset",
            "MVS Dataset",
            "Strecha Dataset",
            "Recently, Hpatches Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://github.com/rmitra/PS-Dataset"
        ]
    },
    {
        "Title": "Detect, Quantify, and Incorporate Dataset Bias: A Neuroimaging Analysis  on 12,207 Individuals",
        "Abstract": "Neuroimaging datasets keep growing in size to address increasingly complex medical questions. However, even the largest datasets today alone are too small for training complex models or for finding genome wide associations. A solution is to grow the sample size by merging data across several datasets. However, bias in datasets complicates this approach and includes additional sources of variation in the data instead. In this work, we combine 15 large neuroimaging datasets to study bias. First, we detect bias by demonstrating that scans can be correctly assigned to a dataset with 73.3% accuracy. Next, we introduce metrics to quantify the compatibility across datasets and to create embeddings of neuroimaging sites. Finally, we incorporate the presence of bias for the selection of a training set for predicting autism. For the quantification of the dataset bias, we introduce two metrics: the Bhattacharyya distance between datasets and the age prediction error. The presented embedding of neuroimaging sites provides an interesting new visualization about the similarity of different sites. This could be used to guide the merging of data sources, while limiting the introduction of unwanted variation. Finally, we demonstrate a clear performance increase when incorporating dataset bias for training set selection in autism prediction. Overall, we believe that the growing amount of neuroimaging data necessitates to incorporate data-driven methods for quantifying dataset bias in future analyses.",
        "Authors": [
            "Christian Wachinger",
            "Benjamin Gutierrez Becker",
            "Anna Rieckmann"
        ],
        "Date": "2018-04-28T09:11:34Z",
        "DOI": [],
        "Category": [
            "cs.CV",
            "cs.AI"
        ],
        "Link": "http://arxiv.org/pdf/1804.10764v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Name That Dataset",
            "IXI: Guys IXI: HH Dataset",
            "Left: Dataset",
            "II Dataset",
            "ABIDE I Dataset",
            "Incorporate Dataset",
            "Abstract\u2014 Neuroimaging Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://brain-development.org/ixi-dataset/b"
        ]
    },
    {
        "Title": "DeepMIMO: A Generic Deep Learning Dataset for Millimeter Wave and  Massive MIMO Applications",
        "Abstract": "Machine learning tools are finding interesting applications in millimeter wave (mmWave) and massive MIMO systems. This is mainly thanks to their powerful capabilities in learning unknown models and tackling hard optimization problems. To advance the machine learning research in mmWave/massive MIMO, however, there is a need for a common dataset. This dataset can be used to evaluate the developed algorithms, reproduce the results, set benchmarks, and compare the different solutions. In this work, we introduce the DeepMIMO dataset, which is a generic dataset for mmWave/massive MIMO channels. The DeepMIMO dataset generation framework has two important features. First, the DeepMIMO channels are constructed based on accurate ray-tracing data obtained from Remcom Wireless InSite. The DeepMIMO channels, therefore, capture the dependence on the environment geometry/materials and transmitter/receiver locations, which is essential for several machine learning applications. Second, the DeepMIMO dataset is generic/parameterized as the researcher can adjust a set of system and channel parameters to tailor the generated DeepMIMO dataset for the target machine learning application. The DeepMIMO dataset can then be completely defined by the (i) the adopted ray-tracing scenario and (ii) the set of parameters, which enables the accurate definition and reproduction of the dataset. In this paper, an example DeepMIMO dataset is described based on an outdoor ray-tracing scenario of 18 base stations and more than one million users. The paper also shows how this dataset can be used in an example deep learning application of mmWave beam prediction.",
        "Authors": [
            "Ahmed Alkhateeb"
        ],
        "Date": "2019-02-18T07:44:08Z",
        "DOI": [],
        "Category": [
            "cs.IT",
            "eess.SP",
            "math.IT"
        ],
        "Link": "http://arxiv.org/pdf/1902.06435v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Deep- MIMO Dataset",
            "Deep Learning Dataset",
            "Generic Deep Learning Dataset",
            "MIMO Dataset",
            "DeepMIMO Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://www.DeepMIMO.net[2",
            "http://www.remcom.com/wireless-insite.[3"
        ]
    },
    {
        "Title": "ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street  Views",
        "Abstract": "In this paper, we introduce the ShopSign dataset, which is a newly developed natural scene text dataset of Chinese shop signs in street views. Although a few scene text datasets are already publicly available (e.g. ICDAR2015, COCO-Text), there are few images in these datasets that contain Chinese texts/characters. Hence, we collect and annotate the ShopSign dataset to advance research in Chinese scene text detection and recognition.   The new dataset has three distinctive characteristics: (1) large-scale: it contains 25,362 Chinese shop sign images, with a total number of 196,010 text-lines. (2) diversity: the images in ShopSign were captured in different scenes, from downtown to developing regions, using more than 50 different mobile phones. (3) difficulty: the dataset is very sparse and imbalanced. It also includes five categories of hard images (mirror, wooden, deformed, exposed and obscure). To illustrate the challenges in ShopSign, we run baseline experiments using state-of-the-art scene text detection methods (including CTPN, TextBoxes++ and EAST), and cross-dataset validation to compare their corresponding performance on the related datasets such as CTW, RCTW and ICPR 2018 MTWI challenge dataset.   The sample images and detailed descriptions of our ShopSign dataset are publicly available at: https://github.com/chongshengzhang/shopsign.",
        "Authors": [
            "Chongsheng Zhang",
            "Guowen Peng",
            "Yuefeng Tao"
        ],
        "Date": "2019-03-25T15:52:32Z",
        "DOI": [],
        "Category": [
            "cs.CV",
            "I.7.5"
        ],
        "Link": "http://arxiv.org/pdf/1903.10412v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Diverse Scene Text Dataset",
            "Therefore, ShopSign Dataset",
            "ShopSign Dataset",
            "Shop- Sign Dataset",
            "RCTW/CTW/MTWI Dataset"
        ],
        "Possible_Dataset_Links": []
    },
    {
        "Title": "A New Stereo Benchmarking Dataset for Satellite Images",
        "Abstract": "In order to facilitate further research in stereo reconstruction with multi-date satellite images, the goal of this paper is to provide a set of stereo-rectified images and the associated groundtruthed disparities for 10 AOIs (Area of Interest) drawn from two sources: 8 AOIs from IARPA's MVS Challenge dataset and 2 AOIs from the CORE3D-Public dataset. The disparities were groundtruthed by first constructing a fused DSM from the stereo pairs and by aligning 30 cm LiDAR with the fused DSM. Unlike the existing benckmarking datasets, we have also carried out a quantitative evaluation of our groundtruthed disparities using human annotated points in two of the AOIs. Additionally, the rectification accuracy in our dataset is comparable to the same in the existing state-of-the-art stereo datasets. In general, we have used the WorldView-3 (WV3) images for the dataset, the exception being the UCSD area for which we have used both WV3 and WorldView-2 (WV2) images. All of the dataset images are now in the public domain. Since multi-date satellite images frequently include images acquired in different seasons (which creates challenges in finding corresponding pairs of pixels for stereo), our dataset also includes for each image a building mask over which the disparities estimated by stereo should prove reliable. Additional metadata included in the dataset includes information about each image's acquisition date and time, the azimuth and elevation angles of the camera, and the intersection angles for the two views in a stereo pair. Also included in the dataset are both quantitative and qualitative analyses of the accuracy of the groundtruthed disparity maps. Our dataset is available for download at \\url{https://engineering.purdue.edu/RVL/Database/SatStereo/index.html}",
        "Authors": [
            "Sonali Patil",
            "Bharath Comandur",
            "Tanmay Prakash"
        ],
        "Date": "2019-07-09T20:41:08Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1907.04404v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "CORE3D-Public Dataset",
            "Jacksonville Dataset",
            "That Dataset",
            "Middlebury Dataset",
            "New Stereo Benchmarking Dataset",
            "IARPA\u2019s MVS Challenge Dataset",
            "In German Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://www.jhuapl.edu/pubgeo/170807",
            "https://engineering.purdue.edu/"
        ]
    },
    {
        "Title": "Challenging More Updates: Towards Anonymous Re-publication of Fully  Dynamic Datasets",
        "Abstract": "Most existing anonymization work has been done on static datasets, which have no update and need only one-time publication. Recent studies consider anonymizing dynamic datasets with external updates: the datasets are updated with record insertions and/or deletions. This paper addresses a new problem: anonymous re-publication of datasets with internal updates, where the attribute values of each record are dynamically updated. This is an important and challenging problem for attribute values of records are updating frequently in practice and existing methods are unable to deal with such a situation.   We initiate a formal study of anonymous re-publication of dynamic datasets with internal updates, and show the invalidation of existing methods. We introduce theoretical definition and analysis of dynamic datasets, and present a general privacy disclosure framework that is applicable to all anonymous re-publication problems. We propose a new counterfeited generalization principle alled m-Distinct to effectively anonymize datasets with both external updates and internal updates. We also develop an algorithm to generalize datasets to meet m-Distinct. The experiments conducted on real-world data demonstrate the effectiveness of the proposed solution.",
        "Authors": [
            "Feng Li",
            "Shuigeng Zhou"
        ],
        "Date": "2008-06-28T16:24:03Z",
        "DOI": [],
        "Category": [
            "cs.DB"
        ],
        "Link": "http://arxiv.org/pdf/0806.4703v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Dynamic Dataset",
            "Risk) Suppose Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://arxiv.org/list/cs.DB/recent\f'represent",
            "http://ipums.org, whichi"
        ]
    },
    {
        "Title": "Collecting and Annotating the Large Continuous Action Dataset",
        "Abstract": "We make available to the community a new dataset to support action-recognition research. This dataset is different from prior datasets in several key ways. It is significantly larger. It contains streaming video with long segments containing multiple action occurrences that often overlap in space and/or time. All actions were filmed in the same collection of backgrounds so that background gives little clue as to action class. We had five humans replicate the annotation of temporal extent of action occurrences labeled with their class and measured a surprisingly low level of intercoder agreement. A baseline experiment shows that recent state-of-the-art methods perform poorly on this dataset. This suggests that this will be a challenging dataset to foster advances in action-recognition research. This manuscript serves to describe the novel content and characteristics of the LCA dataset, present the design decisions made when filming the dataset, and document the novel methods employed to annotate the dataset.",
        "Authors": [
            "Daniel Paul Barrett",
            "Ran Xu",
            "Haonan Yu"
        ],
        "Date": "2015-11-18T19:16:58Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1511.05914v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "VIRAT Dataset",
            "That Dataset",
            "Benchmarking Dataset",
            "Action Dataset",
            "Large Continuous Action Dataset",
            "Y2 Evaluation Dataset",
            "LCA Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://www.visint.org/"
        ]
    },
    {
        "Title": "The WILDTRACK Multi-Camera Person Dataset",
        "Abstract": "People detection methods are highly sensitive to the perpetual occlusions among the targets. As multi-camera set-ups become more frequently encountered, joint exploitation of the across views information would allow for improved detection performances. We provide a large-scale HD dataset named WILDTRACK which finally makes advanced deep learning methods applicable to this problem. The seven-static-camera set-up captures realistic and challenging scenarios of walking people.   Notably, its camera calibration with jointly high-precision projection widens the range of algorithms which may make use of this dataset. In aim to help accelerate the research on automatic camera calibration, such annotations also accompany this dataset.   Furthermore, the rich-in-appearance visual context of the pedestrian class makes this dataset attractive for monocular pedestrian detection as well, since: the HD cameras are placed relatively close to the people, and the size of the dataset further increases seven-fold.   In summary, we overview existing multi-camera datasets and detection methods, enumerate details of our dataset, and we benchmark multi-camera state of the art detectors on this new dataset.",
        "Authors": [
            "Tatjana Chavdarova",
            "Pierre Baqu\u00e9",
            "St\u00e9phane Bouquet"
        ],
        "Date": "2017-07-28T16:05:06Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1707.09299v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Daimler Dataset",
            "EPFL-RLC Dataset",
            "WILDTRACK Dataset",
            "DukeMTMC Dataset",
            "INRIA Dataset",
            "Pets2009: Dataset",
            "Related Dataset",
            "WILDTRACK Multi-Camera Person Dataset",
            "HD Dataset",
            "KITTI Dataset",
            "Daimler-stereo Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://dx.doi.org/10.1007/s10851-012-0405-4.",
            "http://arxiv.org/abs/1702.04593.",
            "https://en.wikipedia.org/w/index.php?title=Bundle_2017a.adjustment&oldid=770262831",
            "http://cvlab.epfl.ch/data/wildtrack3\f",
            "https://en.wikipedia.org/w/index.php?title=Pinhole_camera_model&oldid=782167557",
            "http://pedestriantag.epfl.ch/3",
            "http://dx.doi.org/10.1016/j.patcog.2014.12.004.",
            "http://ceres-solver.",
            "https://github.com/cvlab-epfl/multicam-gt16"
        ]
    },
    {
        "Title": "Constructing a Natural Language Inference Dataset using Generative  Neural Networks",
        "Abstract": "Natural Language Inference is an important task for Natural Language Understanding. It is concerned with classifying the logical relation between two sentences. In this paper, we propose several text generative neural networks for generating text hypothesis, which allows construction of new Natural Language Inference datasets. To evaluate the models, we propose a new metric -- the accuracy of the classifier trained on the generated dataset. The accuracy obtained by our best generative model is only 2.7% lower than the accuracy of the classifier trained on the original, human crafted dataset. Furthermore, the best generated dataset combined with the original dataset achieves the highest accuracy. The best model learns a mapping embedding for each training example. By comparing various metrics we show that datasets that obtain higher ROUGE or METEOR scores do not necessarily yield higher classification accuracies. We also provide analysis of what are the characteristics of a good dataset including the distinguishability of the generated datasets from the original one.",
        "Authors": [
            "Janez Starc",
            "Dunja Mladeni\u0107"
        ],
        "Date": "2016-07-20T16:59:21Z",
        "DOI": [],
        "Category": [
            "cs.AI",
            "cs.CL",
            "cs.NE"
        ],
        "Link": "http://arxiv.org/pdf/1607.06025v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Each Dataset",
            "Natural Language Inference Dataset",
            "After SNLI Dataset",
            "SNLI Dataset",
            "Original Dataset",
            "NLI Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://keras.io",
            "http://nlp.stanford.edu/data/glove.6B.zip",
            "http://github.com/jstarc/nli_generation. It is based on li-brarie"
        ]
    },
    {
        "Title": "AI Challenger : A Large-scale Dataset for Going Deeper in Image  Understanding",
        "Abstract": "Significant progress has been achieved in Computer Vision by leveraging large-scale image datasets. However, large-scale datasets for complex Computer Vision tasks beyond classification are still limited. This paper proposed a large-scale dataset named AIC (AI Challenger) with three sub-datasets, human keypoint detection (HKD), large-scale attribute dataset (LAD) and image Chinese captioning (ICC). In this dataset, we annotate class labels (LAD), keypoint coordinate (HKD), bounding box (HKD and LAD), attribute (LAD) and caption (ICC). These rich annotations bridge the semantic gap between low-level images and high-level concepts. The proposed dataset is an effective benchmark to evaluate and improve different computational methods. In addition, for related tasks, others can also use our dataset as a new resource to pre-train their models.",
        "Authors": [
            "Jiahong Wu",
            "He Zheng",
            "Bo Zhao"
        ],
        "Date": "2017-11-17T09:58:20Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1711.06475v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "ICC Dataset",
            "MSCOCO Dataset",
            "Large-scale Dataset",
            "Large-scale Attribute Dataset",
            "AwA Dataset",
            "Caltech-UCSD Birds-200-2011 Dataset",
            "HKD Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://github.com",
            "https://challenger.ai/3",
            "https://github.com/AIChallenger/AI_Challenger"
        ]
    },
    {
        "Title": "Dimensionality Reduction in Deep Learning for Chest X-Ray Analysis of  Lung Cancer",
        "Abstract": "Efficiency of some dimensionality reduction techniques, like lung segmentation, bone shadow exclusion, and t-distributed stochastic neighbor embedding (t-SNE) for exclusion of outliers, is estimated for analysis of chest X-ray (CXR) 2D images by deep learning approach to help radiologists identify marks of lung cancer in CXR. Training and validation of the simple convolutional neural network (CNN) was performed on the open JSRT dataset (dataset #01), the JSRT after bone shadow exclusion - BSE-JSRT (dataset #02), JSRT after lung segmentation (dataset #03), BSE-JSRT after lung segmentation (dataset #04), and segmented BSE-JSRT after exclusion of outliers by t-SNE method (dataset #05). The results demonstrate that the pre-processed dataset obtained after lung segmentation, bone shadow exclusion, and filtering out the outliers by t-SNE (dataset #05) demonstrates the highest training rate and best accuracy in comparison to the other pre-processed datasets.",
        "Authors": [
            "Yu. Gordienko",
            "Yu. Kochura",
            "O. Alienin"
        ],
        "Date": "2018-01-19T17:15:25Z",
        "DOI": "10.1109/ICACI.2018.8377579",
        "Category": [
            "cs.LG",
            "cs.CY"
        ],
        "Link": "http://arxiv.org/pdf/1801.06495v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Both Dataset",
            "BSE-JSRT Dataset",
            "JSRT Dataset",
            "ChestX-ray14 Dataset",
            "X-ray Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://www.kaggle.com/philschmidt). [16"
        ]
    },
    {
        "Title": "DeepScores -- A Dataset for Segmentation, Detection and Classification  of Tiny Objects",
        "Abstract": "We present the DeepScores dataset with the goal of advancing the state-of-the-art in small objects recognition, and by placing the question of object recognition in the context of scene understanding. DeepScores contains high quality images of musical scores, partitioned into 300,000 sheets of written music that contain symbols of different shapes and sizes. With close to a hundred millions of small objects, this makes our dataset not only unique, but also the largest public dataset. DeepScores comes with ground truth for object classification, detection and semantic segmentation. DeepScores thus poses a relevant challenge for computer vision in general, beyond the scope of optical music recognition (OMR) research. We present a detailed statistical analysis of the dataset, comparing it with other computer vision datasets like Caltech101/256, PASCAL VOC, SUN, SVHN, ImageNet, MS-COCO, smaller computer vision datasets, as well as with other OMR datasets. Finally, we provide baseline performances for object classification and give pointers to future research based on this dataset.",
        "Authors": [
            "Lukas Tuggener",
            "Ismail Elezi",
            "J\u00fcrgen Schmidhuber"
        ],
        "Date": "2018-03-27T14:44:45Z",
        "DOI": [],
        "Category": [
            "cs.CV",
            "cs.LG"
        ],
        "Link": "http://arxiv.org/pdf/1804.00525v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "OMR Dataset",
            "Further OMR Dataset",
            "ImageNet Dataset",
            "MS-COCO Dataset",
            "MuseScore Monophonic MusicXML Dataset",
            "HOMUS Dataset",
            "SVHN Dataset",
            "Moreover, Dataset",
            "DeepScores Dataset",
            "MUSCIMA++ Dataset",
            "Handwritten Online Musical Symbols Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://arxiv.org/abs/1707.04877,2017.[5",
            "https://tuggeluk.github.io/deepscores/",
            "https://musescore.com.",
            "https://apacha.github.io/OMR-Datasets/.",
            "http://lilypond.org/."
        ]
    },
    {
        "Title": "Neural models of factuality",
        "Abstract": "We present two neural models for event factuality prediction, which yield significant performance gains over previous models on three event factuality datasets: FactBank, UW, and MEANTIME. We also present a substantial expansion of the It Happened portion of the Universal Decompositional Semantics dataset, yielding the largest event factuality dataset to date. We report model results on this extended factuality dataset as well.",
        "Authors": [
            "Rachel Rudinger",
            "Aaron Steven White",
            "Benjamin Van Durme"
        ],
        "Date": "2018-04-06T22:11:17Z",
        "DOI": [],
        "Category": [
            "cs.CL"
        ],
        "Link": "http://arxiv.org/pdf/1804.02472v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "UDS-IH2 Dataset",
            "Uni\ufb01ed Factuality Dataset",
            "EFP Dataset",
            "UDS-IH1 Dataset",
            "UW Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://universaldependencies.github.io/docs/ .",
            "http://web.stanford.edu/group/csli_lnr"
        ]
    },
    {
        "Title": "A Framework for Evaluating Snippet Generation for Dataset Search",
        "Abstract": "Reusing existing datasets is of considerable significance to researchers and developers. Dataset search engines help a user find relevant datasets for reuse. They can present a snippet for each retrieved dataset to explain its relevance to the user's data needs. This emerging problem of snippet generation for dataset search has not received much research attention. To provide a basis for future research, we introduce a framework for quantitatively evaluating the quality of a dataset snippet. The proposed metrics assess the extent to which a snippet matches the query intent and covers the main content of the dataset. To establish a baseline, we adapt four state-of-the-art methods from related fields to our problem, and perform an empirical evaluation based on real-world datasets and queries. We also conduct a user study to verify our findings. The results demonstrate the effectiveness of our evaluation framework, and suggest directions for future research.",
        "Authors": [
            "Xiaxia Wang",
            "Jinchi Chen",
            "Shuxin Li"
        ],
        "Date": "2019-07-02T05:58:18Z",
        "DOI": [],
        "Category": [
            "cs.IR",
            "cs.DB"
        ],
        "Link": "http://arxiv.org/pdf/1907.01183v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Google Dataset",
            "An RDF Dataset",
            "RDF Dataset",
            "Illustrative Dataset",
            "Characterising Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://dmoz-odp.org/",
            "https://old.datahub.io/",
            "https://github.com/chabrowa/data-requests-query-dataset1",
            "http://ws.nju.edu.cn/datasetsearch/evaluation-iswc2019/metrics.zip"
        ]
    },
    {
        "Title": "MIMII Dataset: Sound Dataset for Malfunctioning Industrial Machine  Investigation and Inspection",
        "Abstract": "Factory machinery is prone to failure or breakdown, resulting in significant expenses for companies. Hence, there is a rising interest in machine monitoring using different sensors including microphones. In the scientific community, the emergence of public datasets has led to advancements in acoustic detection and classification of scenes and events, but there are no public datasets that focus on the sound of industrial machines under normal and anomalous operating conditions in real factory environments. In this paper, we present a new dataset of industrial machine sounds that we call a sound dataset for malfunctioning industrial machine investigation and inspection (MIMII dataset). Normal sounds were recorded for different types of industrial machines (i.e., valves, pumps, fans, and slide rails), and to resemble a real-life scenario, various anomalous sounds were recorded (e.g., contamination, leakage, rotating unbalance, and rail damage). The purpose of releasing the MIMII dataset is to assist the machine-learning and signal-processing community with their development of automated facility maintenance. The MIMII dataset is freely available for download at: https://zenodo.org/record/3384388",
        "Authors": [
            "Harsh Purohit",
            "Ryo Tanabe",
            "Kenji Ichige"
        ],
        "Date": "2019-09-20T07:17:34Z",
        "DOI": [],
        "Category": [
            "cs.SD",
            "cs.LG",
            "eess.AS",
            "stat.ML"
        ],
        "Link": "http://arxiv.org/pdf/1909.09347v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "MIMII Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://www.sifi.co.jp/system/modules/pico/index.php?conten",
            "https://zenodo.org/record/3384388. This datasetcontain"
        ]
    },
    {
        "Title": "Automatically Building Face Datasets of New Domains from Weakly Labeled  Data with Pretrained Models",
        "Abstract": "Training data are critical in face recognition systems. However, labeling a large scale face data for a particular domain is very tedious. In this paper, we propose a method to automatically and incrementally construct datasets from massive weakly labeled data of the target domain which are readily available on the Internet under the help of a pretrained face model. More specifically, given a large scale weakly labeled dataset in which each face image is associated with a label, i.e. the name of an identity, we create a graph for each identity with edges linking matched faces verified by the existing model under a tight threshold. Then we use the maximal subgraph as the cleaned data for that identity. With the cleaned dataset, we update the existing face model and use the new model to filter the original dataset to get a larger cleaned dataset. We collect a large weakly labeled dataset containing 530,560 Asian face images of 7,962 identities from the Internet, which will be published for the study of face recognition. By running the filtering process, we obtain a cleaned datasets (99.7+% purity) of size 223,767 (recall 70.9%). On our testing dataset of Asian faces, the model trained by the cleaned dataset achieves recognition rate 93.1%, which obviously outperforms the model trained by the public dataset CASIA whose recognition rate is 85.9%.",
        "Authors": [
            "Shengyong Ding",
            "Junyu Wu",
            "Wei Xu"
        ],
        "Date": "2016-11-24T09:11:21Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1611.08107v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Typical Dataset",
            "M Output: Clean Dataset",
            "Face Datasets Face Dataset",
            "Training Dataset"
        ],
        "Possible_Dataset_Links": []
    },
    {
        "Title": "Creating A Multi-track Classical Musical Performance Dataset for  Multimodal Music Analysis: Challenges, Insights, and Applications",
        "Abstract": "We introduce a dataset for facilitating audio-visual analysis of music performances. The dataset comprises 44 simple multi-instrument classical music pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation files including frame-level and note-level transcriptions. We describe our methodology for the creation of the dataset, particularly highlighting our approaches for addressing the challenges involved in maintaining synchronization and expressiveness. We demonstrate the high quality of synchronization achieved with our proposed approach by comparing the dataset with existing widely-used music audio datasets.   We anticipate that the dataset will be useful for the development and evaluation of existing music information retrieval (MIR) tasks, as well as for novel multi-modal tasks. We benchmark two existing MIR tasks (multi-pitch analysis and score-informed source separation) on the dataset and compare with other existing music audio datasets. Additionally, we consider two novel multi-modal MIR tasks (visually informed multi-pitch analysis and polyphonic vibrato analysis) enabled by the dataset and provide evaluation measures and baseline systems for future comparisons (from our recent work). Finally, we propose several emerging research directions that the dataset enables.",
        "Authors": [
            "Bochen Li",
            "Xinzhao Liu",
            "Karthik Dinesh"
        ],
        "Date": "2016-12-27T20:27:24Z",
        "DOI": "10.1109/TMM.2018.2856090",
        "Category": [
            "cs.MM",
            "cs.SD"
        ],
        "Link": "http://arxiv.org/pdf/1612.08727v3.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "C4S Dataset",
            "RWC Dataset",
            "Both Dataset",
            "GB Dataset",
            "Structural Segmentation Multitrack Dataset",
            "URMP Dataset",
            "MASS Dataset",
            "TRIOS Dataset",
            "Bach10 Dataset",
            "Mixploration Dataset",
            "WWQ Dataset",
            "Guitar Dataset",
            "Multi-track Classical Music Performance Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://www.apple.com/final-cut-pro/",
            "http://doi.org/10.5061/dryad.ng3r749[53",
            "http://www.esm.rochester.edu/concerts/halls/hatch/8ground-trut",
            "http://www.8notes.com\f7",
            "http://www.avid.com/en/sibelius, accessed: Sept, 2014.[46",
            "http://www.disklavier.com\f3",
            "http://www.audacityteam.org, accessed: Aug, 2015.[47",
            "http://www.music-ir.org/mirex/wiki/MIREX HOME\fguita",
            "http://expandedramblings.com/index.php/youtube-statistics/4/,online",
            "http://www.mtg.upf.edu/static/mass/resources, 2008.[31"
        ]
    },
    {
        "Title": "Comparing Dataset Characteristics that Favor the Apriori, Eclat or  FP-Growth Frequent Itemset Mining Algorithms",
        "Abstract": "Frequent itemset mining is a popular data mining technique. Apriori, Eclat, and FP-Growth are among the most common algorithms for frequent itemset mining. Considerable research has been performed to compare the relative performance between these three algorithms, by evaluating the scalability of each algorithm as the dataset size increases. While scalability as data size increases is important, previous papers have not examined the performance impact of similarly sized datasets that contain different itemset characteristics. This paper explores the effects that two dataset characteristics can have on the performance of these three frequent itemset algorithms. To perform this empirical analysis, a dataset generator is created to measure the effects of frequent item density and the maximum transaction size on performance. The generated datasets contain the same number of rows. This provides some insight into dataset characteristics that are conducive to each algorithm. The results of this paper's research demonstrate Eclat and FP-Growth both handle increases in maximum transaction size and frequent itemset density considerably better than the Apriori algorithm.   This paper explores the effects that two dataset characteristics can have on the performance of these three frequent itemset algorithms. To perform this empirical analysis, a dataset generator is created to measure the effects of frequent item density and the maximum transaction size on performance. The generated datasets contain the same number of rows. This provides some insight into dataset characteristics that are conducive to each algorithm. The results of this paper's research demonstrate Eclat and FP-Growth both handle increases in maximum transaction size and frequent itemset density considerably better than the Apriori algorithm.",
        "Authors": [
            "Jeff Heaton"
        ],
        "Date": "2017-01-30T12:34:02Z",
        "DOI": [],
        "Category": [
            "cs.DB",
            "cs.AI"
        ],
        "Link": "http://arxiv.org/pdf/1701.09042v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "DATASET GENERATION Generated Dataset",
            "Comparing Dataset",
            "EFFECTS OF DATASET DENSITY Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://mloss.org/software/view/294/.[8",
            "http://www.ics.uci.edu/$\\sim$mlearn/",
            "https://github.com/jeffheaton/papers, accessed: 2016-01-31.[9",
            "http://www.jstatsoft.org/v14/i15/[4"
        ]
    },
    {
        "Title": "Large-scale Datasets: Faces with Partial Occlusions and Pose Variations  in the Wild",
        "Abstract": "Face detection methods have relied on face datasets for training. However, existing face datasets tend to be in small scales for face learning in both constrained and unconstrained environments. In this paper, we first introduce our large-scale image datasets, Large-scale Labeled Face (LSLF) and noisy Large-scale Labeled Non-face (LSLNF). Our LSLF dataset consists of a large number of unconstrained multi-view and partially occluded faces. The faces have many variations in color and grayscale, image quality, image resolution, image illumination, image background, image illusion, human face, cartoon face, facial expression, light and severe partial facial occlusion, make up, gender, age, and race. Many of these faces are partially occluded with accessories such as tattoos, hats, glasses, sunglasses, hands, hair, beards, scarves, microphones, or other objects or persons. The LSLF dataset is currently the largest labeled face image dataset in the literature in terms of the number of labeled images and the number of individuals compared to other existing labeled face image datasets. Second, we introduce our CrowedFaces and CrowedNonFaces image datasets. The crowedFaces and CrowedNonFaces datasets include faces and non-faces images from crowed scenes. These datasets essentially aim for researchers to provide a large number of training examples with many variations for large scale face learning and face recognition tasks.",
        "Authors": [
            "Tarik Alafif",
            "Zeyad Hailat",
            "Melih Aslan"
        ],
        "Date": "2017-06-27T07:04:51Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1706.08690v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "LSLF Dataset",
            "LFW Dataset",
            "FERET Dataset",
            "Multi-PIE Dataset",
            "Crowd- NonFaces Dataset",
            "CrowdFaces Dataset",
            "Extended Yale B Dataset",
            "WebV-Cele Dataset",
            "CrowdNonFaces Dataset",
            "CAS-PEAL Dataset",
            "FRGC Dataset",
            "LSLNF Dataset",
            "CrowedNonFaces Dataset"
        ],
        "Possible_Dataset_Links": []
    },
    {
        "Title": "A Longitudinal Assessment of the Persistence of Twitter Datasets",
        "Abstract": "With social media datasets being increasingly shared by researchers, it also presents the caveat that those datasets are not always completely replicable. Having to adhere to requirements of platforms like Twitter, researchers cannot release the raw data and instead have to release a list of unique identifiers, which others can then use to recollect the data from the platform themselves. This leads to the problem that subsets of the data may no longer be available, as content can be deleted or user accounts deactivated. To quantify the impact of content deletion in the replicability of datasets in a long term, we perform a longitudinal analysis of the persistence of 30 Twitter datasets, which include over 147 million tweets. Having the original datasets collected between 2012 and 2016, and recollecting them later by using the tweet IDs, we look at four different factors that quantify the extent to which recollected datasets resemble original ones: completeness, representativity, similarity and changingness. Even though the ratio of available tweets keeps decreasing as the dataset gets older, we find that the textual content of the recollected subset is still largely representative of the whole dataset that was originally collected. The representativity of the metadata, however, keeps decreasing over time, both because the dataset shrinks and because certain metadata, such as the users' number of followers, keeps changing. Our study has important implications for researchers sharing and using publicly shared Twitter datasets in their research.",
        "Authors": [
            "Arkaitz Zubiaga"
        ],
        "Date": "2017-09-26T18:00:33Z",
        "DOI": [],
        "Category": [
            "cs.DL"
        ],
        "Link": "http://arxiv.org/pdf/1709.09186v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Twitter Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://archive.org/details/twitterstream",
            "https://figshare.com/articles/Twitter_event_datasets_2012-2016_/51004608",
            "https://twitter.com/tos2",
            "https://dev.twitter.com/rest/reference/get/statuses/lookup5",
            "https://www.politwoops.eu/4",
            "https://dev.twitter.com/rest/reference/get/statuses/lookup4\f",
            "https://dev.twitter.com/overview/terms/agreement-and-policy2\f"
        ]
    },
    {
        "Title": "An empirical study of public data quality problems in cross project  defect prediction",
        "Abstract": "Background: Two public defect data, including Jureczko and NASA datasets, have been widely used in cross project defect prediction (CPDP). The quality of defect data have been reported as an important factor influencing the defect prediction performance and Shepperd et al. have researched the data quality problems in NASA datasets. However, up to now, there is no research focusing on the quality problems of Jureczko datasets which are most widely used in CPDP. Aims: In this paper, we intend to investigate the problems of identical and inconsistent cases in Jureczko datasets and validate whether removing these problematic cases will make a difference to defect prediction performance in CPDP. Method: The problems of identical and inconsistent cases are reported from two aspects, respectively in each individual dataset and in a pair of datasets from different releases of a software project. Then a cleaned version of Jureczko datasets is provided by removing duplicate and inconsistent cases. Finally three training data selection methods are employed to compare the defect prediction performance of cleaned datasets with that of original datasets. Results: The experimental results in terms of AUC and F-Measure show that most datasets obtain very different defect prediction performance. Conclusions: It is very necessary to study the data quality problems in CPDP and the cleaned Jureczko datasets may provide more reliable defect prediction performance in CPDP.",
        "Authors": [
            "Zhongbin Sun",
            "Junqi Li",
            "Heli Sun"
        ],
        "Date": "2018-05-28T06:57:40Z",
        "DOI": [],
        "Category": [
            "cs.SE"
        ],
        "Link": "http://arxiv.org/pdf/1805.10787v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Jureczko Dataset",
            "NASA Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://gr.xjtu.edu.cn/web/zhongbin725/datasets.",
            "http://openscience.us/repo/defect/ck/"
        ]
    },
    {
        "Title": "Exploring large scale public medical image datasets",
        "Abstract": "Rationale and Objectives: Medical artificial intelligence systems are dependent on well characterised large scale datasets. Recently released public datasets have been of great interest to the field, but pose specific challenges due to the disconnect they cause between data generation and data usage, potentially limiting the utility of these datasets.   Materials and Methods: We visually explore two large public datasets, to determine how accurate the provided labels are and whether other subtle problems exist. The ChestXray14 dataset contains 112,120 frontal chest films, and the MURA dataset contains 40,561 upper limb radiographs. A subset of around 700 images from both datasets was reviewed by a board-certified radiologist, and the quality of the original labels was determined.   Results: The ChestXray14 labels did not accurately reflect the visual content of the images, with positive predictive values mostly between 10% and 30% lower than the values presented in the original documentation. There were other significant problems, with examples of hidden stratification and label disambiguation failure. The MURA labels were more accurate, but the original normal/abnormal labels were inaccurate for the subset of cases with degenerative joint disease, with a sensitivity of 60% and a specificity of 82%.   Conclusion: Visual inspection of images is a necessary component of understanding large image datasets. We recommend that teams producing public datasets should perform this important quality control procedure and include a thorough description of their findings, along with an explanation of the data generating procedures and labelling rules, in the documentation for their datasets.",
        "Authors": [
            "Luke Oakden-Rayner"
        ],
        "Date": "2019-07-30T03:09:27Z",
        "DOI": [],
        "Category": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "Link": "http://arxiv.org/pdf/1907.12720v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Each Dataset",
            "Open-i Dataset",
            "ChestXray14 Dataset",
            "CXR14 Dataset",
            "MURA Dataset"
        ],
        "Possible_Dataset_Links": []
    },
    {
        "Title": "FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from  Single RGB Images",
        "Abstract": "Estimating 3D hand pose from single RGB images is a highly ambiguous problem that relies on an unbiased training dataset. In this paper, we analyze cross-dataset generalization when training on existing datasets. We find that approaches perform well on the datasets they are trained on, but do not generalize to other datasets or in-the-wild scenarios. As a consequence, we introduce the first large-scale, multi-view hand dataset that is accompanied by both 3D hand pose and shape annotations. For annotating this real-world dataset, we propose an iterative, semi-automated `human-in-the-loop' approach, which includes hand fitting optimization to infer both the 3D pose and shape for each sample. We show that methods trained on our dataset consistently perform well when tested on other datasets. Moreover, the dataset allows us to train a network that predicts the full articulated hand shape from a single RGB image. The evaluation set can serve as a benchmark for articulated hand shape estimation.",
        "Authors": [
            "Christian Zimmermann",
            "Duygu Ceylan",
            "Jimei Yang"
        ],
        "Date": "2019-09-10T08:29:58Z",
        "DOI": [],
        "Category": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "Link": "http://arxiv.org/pdf/1909.04349v3.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Synthetic Dataset",
            "RHD Dataset",
            "Panoptic Dataset",
            "Related Work Since Dataset",
            "Hand Pose Dataset",
            "Shape Dataset",
            "FreiHAND Dataset",
            "FPA Dataset",
            "RGB Dataset",
            "State-of-the-art Dataset",
            "Rendered Hand Pose Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://lmb.informatik.uni-freiburg.de/projects/freihand/9102",
            "https://arxiv.org/abs/1705.01389. 1, 3, 4, 6, 11",
            "https://www.leapmotion.com. 3[23"
        ]
    },
    {
        "Title": "A*3D Dataset: Towards Autonomous Driving in Challenging Environments",
        "Abstract": "With the increasing global popularity of self-driving cars, there is an immediate need for challenging real-world datasets for benchmarking and training various computer vision tasks such as 3D object detection. Existing datasets either represent simple scenarios or provide only day-time data. In this paper, we introduce a new challenging A*3D dataset which consists of RGB images and LiDAR data with significant diversity of scene, time, and weather. The dataset consists of high-density images ($\\approx~10$ times more than the pioneering KITTI dataset), heavy occlusions, a large number of night-time frames ($\\approx~3$ times the nuScenes dataset), addressing the gaps in the existing datasets to push the boundaries of tasks in autonomous driving research to more challenging highly diverse environments. The dataset contains $39\\text{K}$ frames, $7$ classes, and $230\\text{K}$ 3D object annotations. An extensive 3D object detection benchmark evaluation on the A*3D dataset for various attributes such as high density, day-time/night-time, gives interesting insights into the advantages and limitations of training and testing 3D object detection in real-world setting.",
        "Authors": [
            "Quang-Hieu Pham",
            "Pierre Sevestre",
            "Ramanpreet Singh Pahwa"
        ],
        "Date": "2019-09-17T01:19:04Z",
        "DOI": [],
        "Category": [
            "cs.CV",
            "cs.RO"
        ],
        "Link": "http://arxiv.org/pdf/1909.07541v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Moreover, A*3D Dataset",
            "Hence A*3D Dataset",
            "Existing Dataset",
            "LiDAR Dataset",
            "KITTI Dataset",
            "A*3D Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://github.com/I2RDL2/ASTAR-3D for non-commercia"
        ]
    },
    {
        "Title": "Identifying and Improving Dataset References in Social Sciences Full  Texts",
        "Abstract": "Scientific full text papers are usually stored in separate places than their underlying research datasets. Authors typically make references to datasets by mentioning them for example by using their titles and the year of publication. However, in most cases explicit links that would provide readers with direct access to referenced datasets are missing. Manually detecting references to datasets in papers is time consuming and requires an expert in the domain of the paper. In order to make explicit all links to datasets in papers that have been published already, we suggest and evaluate a semi-automatic approach for finding references to datasets in social sciences papers. Our approach does not need a corpus of papers (no cold start problem) and it performs well on a small test corpus (gold standard). Our approach achieved an F-measure of 0.84 for identifying references in full texts and an F-measure of 0.83 for finding correct matches of detected references in the da|ra dataset registry.",
        "Authors": [
            "Behnam Ghavimi",
            "Philipp Mayr",
            "Sahar Vahdati"
        ],
        "Date": "2016-03-06T01:09:08Z",
        "DOI": [],
        "Category": [
            "cs.DL",
            "cs.IR"
        ],
        "Link": "http://arxiv.org/pdf/1603.01774v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Improving Dataset",
            "Matching Dataset",
            "Detecting Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://www.da-ra.de7",
            "http://www.gesis.org/en/publications/journals/mda/6",
            "http://da-ra.de/oaip/\f5"
        ]
    },
    {
        "Title": "UMDFaces: An Annotated Face Dataset for Training Deep Networks",
        "Abstract": "Recent progress in face detection (including keypoint detection), and recognition is mainly being driven by (i) deeper convolutional neural network architectures, and (ii) larger datasets. However, most of the large datasets are maintained by private companies and are not publicly available. The academic computer vision community needs larger and more varied datasets to make further progress.   In this paper we introduce a new face dataset, called UMDFaces, which has 367,888 annotated faces of 8,277 subjects. We also introduce a new face recognition evaluation protocol which will help advance the state-of-the-art in this area. We discuss how a large dataset can be collected and annotated using human annotators and deep networks. We provide human curated bounding boxes for faces. We also provide estimated pose (roll, pitch and yaw), locations of twenty-one key-points and gender information generated by a pre-trained neural network. In addition, the quality of keypoint annotations has been verified by humans for about 115,000 images. Finally, we compare the quality of the dataset with other publicly available face datasets at similar scales.",
        "Authors": [
            "Ankan Bansal",
            "Anirudh Nanduri",
            "Carlos Castillo"
        ],
        "Date": "2016-11-04T18:37:41Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1611.01484v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "CASIA WebFace Dataset",
            "UMD- Faces Dataset",
            "Megaface Dataset",
            "LFW Dataset",
            "UMDFaces Dataset",
            "IJB-A Dataset",
            "VGGFace Dataset",
            "AFW Dataset",
            "Ours AFLW Dataset",
            "CelebFaces+ Dataset",
            "Similar Dataset",
            "WIDER FACE Dataset",
            "UMDFaces: An Annotated Face Dataset",
            "CA- SIA WebFace Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://www.umdfaces.io",
            "http://www.pascal-network.org/challenges",
            "https://github.com/NikolaiT/GoogleScraper\f3",
            "http://places2.csail.mit.edu. 1[2"
        ]
    },
    {
        "Title": "Combination of PCA with SMOTE Resampling to Boost the Prediction Rate in  Lung Cancer Dataset",
        "Abstract": "Classification algorithms are unable to make reliable models on the datasets with huge sizes. These datasets contain many irrelevant and redundant features that mislead the classifiers. Furthermore, many huge datasets have imbalanced class distribution which leads to bias over majority class in the classification process. In this paper combination of unsupervised dimensionality reduction methods with resampling is proposed and the results are tested on Lung-Cancer dataset. In the first step PCA is applied on Lung-Cancer dataset to compact the dataset and eliminate irrelevant features and in the second step SMOTE resampling is carried out to balance the class distribution and increase the variety of sample domain. Finally, Naive Bayes classifier is applied on the resulting dataset and the results are compared and evaluation metrics are calculated. The experiments show the effectiveness of the proposed method across four evaluation metrics: Overall accuracy, False Positive Rate, Precision, Recall.",
        "Authors": [
            "Mehdi Naseriparsa",
            "Mohammad Mansour Riahi Kashani"
        ],
        "Date": "2014-03-08T08:12:54Z",
        "DOI": "10.5120/13376-0987",
        "Category": [
            "cs.LG",
            "cs.CE"
        ],
        "Link": "http://arxiv.org/pdf/1403.1949v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Compacted Dataset",
            "Lung-Cancer Dataset",
            "Lung- Cancer Dataset",
            "Initial Lung-Cancer Dataset",
            "Lung Cancer Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://www.ics.uci.edu/~mlearn"
        ]
    },
    {
        "Title": "BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis",
        "Abstract": "In this paper we introduce a large-scale hand pose dataset, collected using a novel capture method. Existing datasets are either generated synthetically or captured using depth sensors: synthetic datasets exhibit a certain level of appearance difference from real depth images, and real datasets are limited in quantity and coverage, mainly due to the difficulty to annotate them. We propose a tracking system with six 6D magnetic sensors and inverse kinematics to automatically obtain 21-joints hand pose annotations of depth maps captured with minimal restriction on the range of motion. The capture protocol aims to fully cover the natural hand pose space. As shown in embedding plots, the new dataset exhibits a significantly wider and denser range of hand poses compared to existing benchmarks. Current state-of-the-art methods are evaluated on the dataset, and we demonstrate significant improvements in cross-benchmark performance. We also show significant improvements in egocentric hand pose estimation with a CNN trained on the new dataset.",
        "Authors": [
            "Shanxin Yuan",
            "Qi Ye",
            "Bjorn Stenger"
        ],
        "Date": "2017-04-09T15:00:31Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1704.02612v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "HandNet Dataset",
            "MSRA15 Dataset",
            "MSRC Dataset",
            "Annotation Dataset",
            "Existing Dataset",
            "Graz16 Dataset",
            "Benchmark: Hand Pose Dataset",
            "UCI-EGO Dataset",
            "Egocentric Dataset",
            "ASTAR Dataset",
            "ICVL Dataset",
            "Creating Dataset",
            "NYU Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://www.shapehand.com/shapehand.html.[24",
            "https://www.ascension-tech.com/products/trakstar-2-drivebay-2/.[11",
            "https://click.intel.com/intelrealsense-developer-kit-featuring-sr300.html.[4"
        ]
    },
    {
        "Title": "Automatic Dataset Augmentation",
        "Abstract": "Large scale image dataset and deep convolutional neural network (DCNN) are two primary driving forces for the rapid progress made in generic object recognition tasks in recent years. While lots of network architectures have been continuously designed to pursue lower error rates, few efforts are devoted to enlarge existing datasets due to high labeling cost and unfair comparison issues. In this paper, we aim to achieve lower error rate by augmenting existing datasets in an automatic manner. Our method leverages both Web and DCNN, where Web provides massive images with rich contextual information, and DCNN replaces human to automatically label images under guidance of Web contextual information. Experiments show our method can automatically scale up existing datasets significantly from billions web pages with high accuracy, and significantly improve the performance on object recognition tasks by using the automatically augmented datasets, which demonstrates that more supervisory information has been automatically gathered from the Web. Both the dataset and models trained on the dataset are made publicly available.",
        "Authors": [
            "Yalong Bai",
            "Kuiyuan Yang",
            "Tao Mei"
        ],
        "Date": "2017-08-28T06:22:00Z",
        "DOI": [],
        "Category": [
            "cs.CV"
        ],
        "Link": "http://arxiv.org/pdf/1708.08201v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "ILSVRC-2012 Dataset",
            "Novel Dataset",
            "Each Dataset",
            "Automatic Dataset",
            "V T Dataset",
            "Augmented Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://support.google.com/webmasters/answer/114016?hl=en",
            "http://www.vision.ee.ethz.ch/webvision",
            "https://auto-da.github.io/hav",
            "http://www.bbc.co.uk/nature/life/"
        ]
    },
    {
        "Title": "Paris-Lille-3D: a large and high-quality ground truth urban point cloud  dataset for automatic segmentation and classification",
        "Abstract": "This paper introduces a new Urban Point Cloud Dataset for Automatic Segmentation and Classification acquired by Mobile Laser Scanning (MLS). We describe how the dataset is obtained from acquisition to post-processing and labeling. This dataset can be used to learn classification algorithm, however, given that a great attention has been paid to the split between the different objects, this dataset can also be used to learn the segmentation. The dataset consists of around 2km of MLS point cloud acquired in two cities. The number of points and range of classes make us consider that it can be used to train Deep-Learning methods. Besides we show some results of automatic segmentation and classification. The dataset is available at: http://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/",
        "Authors": [
            "Xavier Roynard",
            "Jean-Emmanuel Deschaud",
            "Fran\u00e7ois Goulette"
        ],
        "Date": "2017-11-30T19:08:52Z",
        "DOI": [],
        "Category": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "Link": "http://arxiv.org/pdf/1712.00032v2.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Oakland Dataset",
            "Point Cloud Dataset",
            "Each Dataset",
            "Available Datasets Numerous Dataset",
            "Paris Dataset",
            "Rue-Madame Dataset",
            "Oxford Robotcar Dataset",
            "Urban Point Cloud Dataset",
            "Oxford RobotCar Dataset",
            "Semantic3D Dataset",
            "Recall Dataset"
        ],
        "Possible_Dataset_Links": [
            "http://www.danielgm.net/cc/9",
            "http://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/.",
            "http://rgp.ign.fr/2",
            "https://www.novatel.com/products/software/inertial-explorer/7",
            "http://data.ign.fr/benchmarks/UrbanAnalysis/download/classes.xm"
        ]
    },
    {
        "Title": "Indian Regional Movie Dataset for Recommender Systems",
        "Abstract": "Indian regional movie dataset is the first database of regional Indian movies, users and their ratings. It consists of movies belonging to 18 different Indian regional languages and metadata of users with varying demographics. Through this dataset, the diversity of Indian regional cinema and its huge viewership is captured. We analyze the dataset that contains roughly 10K ratings of 919 users and 2,851 movies using some supervised and unsupervised collaborative filtering techniques like Probabilistic Matrix Factorization, Matrix Completion, Blind Compressed Sensing etc. The dataset consists of metadata information of users like age, occupation, home state and known languages. It also consists of metadata of movies like genre, language, release year and cast. India has a wide base of viewers which is evident by the large number of movies released every year and the huge box-office revenue. This dataset can be used for designing recommendation systems for Indian users and regional movies, which do not, yet, exist. The dataset can be downloaded from \\href{https://goo.gl/EmTPv6}{https://goo.gl/EmTPv6}.",
        "Authors": [
            "Prerna Agarwal",
            "Richa Verma",
            "Angshul Majumdar"
        ],
        "Date": "2018-01-07T16:02:35Z",
        "DOI": [],
        "Category": [
            "cs.IR",
            "cs.AI"
        ],
        "Link": "http://arxiv.org/pdf/1801.02203v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Indian Regional Cinema Dataset",
            "Indian Regional Movie Dataset",
            "Movielens Dataset",
            "MovieLens Dataset",
            "Datasets Dataset"
        ],
        "Possible_Dataset_Links": [
            "https://movielens.org/[2",
            "https://grouplens.org/datasets/movielens/[39",
            "https://arxiv.org/abs/1410.2596.lishe",
            "https://www.clickondetroit.com/entertainment/netflix-steps-up-its-battle-with-amazon-in-india[37",
            "http://arxiv.org/abs/1505.01621[6",
            "http://uis.unesco.org/en/news/record-number-films-produced[10",
            "http://timesofindia.indiatimes.com/companies/amazon-to-join-bollywood-film-industry-hires-consultants-to-create-a-blueprint\\-for-a-hindi-film-studio/articleshow/58176571.cmsfor",
            "http://www.amazon.in[35",
            "https://www.barnesandnoble.com[36",
            "http://www.imdb.com/",
            "https://www.netflix.com/in/[3][4",
            "https://goo.gl/EmTPv6.",
            "http://www.netflixprize.com/[9",
            "http://www.financialexpress.com/industry/technology/the-desi-content-battleground/798145/[38",
            "http://flickscore.iiitd.edu.in[16"
        ]
    },
    {
        "Title": "A new dataset and model for learning to understand navigational  instructions",
        "Abstract": "In this paper, we present a state-of-the-art model and introduce a new dataset for grounded language learning. Our goal is to develop a model that can learn to follow new instructions given prior instruction-perception-action examples. We based our work on the SAIL dataset which consists of navigational instructions and actions in a maze-like environment. The new model we propose achieves the best results to date on the SAIL dataset by using an improved perceptual component that can represent relative positions of objects. We also analyze the problems with the SAIL dataset regarding its size and balance. We argue that performance on a small, fixed-size dataset is no longer a good measure to differentiate state-of-the-art models. We introduce SAILx, a synthetic dataset generator, and perform experiments where the size and balance of the dataset are controlled.",
        "Authors": [
            "Ozan Arkan Can",
            "Deniz Yuret"
        ],
        "Date": "2018-05-21T09:01:31Z",
        "DOI": [],
        "Category": [
            "cs.CL"
        ],
        "Link": "http://arxiv.org/pdf/1805.07952v1.pdf",
        "Archive": "arXiv",
        "Prediction": "Dataset Detected",
        "Dataset_Names": [
            "Synthetic Dataset",
            "Fixed-Sized SAILx Dataset",
            "SAILx Dataset",
            "Paragraph Dataset",
            "SAIL Dataset",
            "Thus Dataset"
        ],
        "Possible_Dataset_Links": []
    }
]